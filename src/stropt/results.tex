\section{Results}
\label{so:sec:results}

Because Martel's approach defers selecting optimal options until the end of
equivalent expression discovery, we developed a method that could produce
exactly the same set of equivalent expressions from the traces computed by
Martel, and has the same time complexity. The difference is that we adopted it
to generate a Pareto frontier from the discovered expressions, instead of only
error bounds.  This allows us to compare \marteltrace{}, \ie~our implementation
of Martel's method, against our methods \frontiertrace{} and \greedytrace{}
discussed in Section~\ref{so:sec:equivalent}.  Figure~\ref{so:fig:martel}
optimizes the expression ${(\vara + \varb)}^2$ using the three methods above,
all using depth limit $3$, and the input ranges are $\vara \in [5, 10]$
and $\varb \in [0, 0.001]$~\cite{martel07}. The IEEE 754 single-precision
floating-point format with rounding to nearest was used for the evaluation
of accuracy and area estimation. The scatter points represent different
implementations of the original expression that have been explored and
analyzed, and the (overlapping) lines denote the Pareto frontiers. In this
example, our methods produce the same Pareto frontier that Martel's method
could discover, while having up to 50\% shorter run time. Because we consider
an accuracy/area trade-off, we find that we can not only have the most accurate
implementation discovered by Martel, but also an option that is only 0.0005\%
less accurate, but uses 7\% fewer \glspl{lut}.

We go beyond the optimization of a small expression, by generating results in
Figure~\ref{so:fig:multi_expr_32} to show that the same technique is applicable
to simultaneous optimization of multiple large expressions. The expressions
$e_1$ and $e_2$, with input ranges $\vara \in [1, 2], \varb \in [10, 20], \varc
\in [10, 200]$ are used as our example:
\begin{equation}
    \begin{aligned}
    e_1 =&
        (\vara + \vara + \varb) \times
        (\vara + \varb + \varb) \times
        (\varb + \varb + \varc) \times {} \\
        &
        (\varb + \varc + \varc) \times
        (\varc + \varc + \vara) \times
        (\varc + \vara + \vara), \\
    e_2 =&
        (1 + \varb + \varc) \times
        (\vara + 1 + \varc) \times
        (\vara + \varb + 1).
    \end{aligned}
\end{equation}

We generated and optimized \gls{rtl} implementations of $e_1$ and
$e_2$ simultaneously using \frontiertrace{} and \greedytrace{}
with the depth limits indicated by the numbers in the legend of
Figure~\ref{so:fig:multi_expr_32}. Note that because the expressions evaluate
to large values, the errors are also relatively large. We set the depth limit
to $2$ and found that \greedytrace{} executes up to $10\times$ faster than
\frontiertrace{}, while discovering a sizable subset of the Pareto frontier of
\frontiertrace{}. Also our methods are significantly faster and more scalable
than \marteltrace{}, because of its poor scalability discussed earlier, our
computer ran out of 8 GB of memory before we could produce any results. If we
normalize the time allowed for each method and compare the performance, we
found that \greedytrace{} with a depth limit $3$ takes takes slightly less time
than \frontiertrace{} with a depth limit $2$, but produces a generally better
Pareto frontier. The alternative implementations of the original expression
provided by the Pareto frontier of \greedytrace{} can either reduce the
\glspl{lut} used by approximately 10\% when accuracy is not crucial, or can
be about 10\% more accurate if resource is not our concern.  It also enables
the ability to choose different trade-off options, such as an implementation
that is 7\% more accurate and uses 7\% fewer \glspl{lut} than the original
expression.

Furthermore, Figure~\ref{so:fig:multi_expr_vary_width} varies the mantissa
width of the floating-point format, and presents the Pareto frontier
of both $e_1$ and $e_2$ together under optimization. Floating-point
formats with mantissa widths ranging from 10 to 112 bits were used to
optimize and evaluate the expressions for both accuracy and area usage. It
turns out that some implementations originally on the Pareto frontier of
Figure~\ref{so:fig:multi_expr_32} are no longer desirable, as by varying the
mantissa width, new implementations are both more accurate and less resource
demanding.

Besides the large example expressions above, Figure~\ref{so:fig:taylor_sin}
and Figure~\ref{so:fig:motzkin} are produced by optimizing expressions with
real applications under single precision. Figure~\ref{so:fig:taylor_sin} shows
the optimization of the Taylor expansion of $\sin(x + y)$, where $x\in[-0.1,
0.1]$ and $y\in[0, 1]$, using \greedytrace{} with a depth limit $3$. The
function $\mathrm{taylor}(f, d)$ indicates the Taylor expansion of function
$f(x, y)$ at $x = y = 0$ with a maximum degree of $d$. For order 5 we reduced
error by more than 60\%. Figure~\ref{so:fig:motzkin} illustrates the results
obtained using the depth limit $3$ with the Motzkin polynomial~\cite{demmel}
$x^6 + y^4 z^2 + y^2 z^4 - 3 x^2 y^2 z^2$, which is known to be difficult to
evaluate accurately, especially using inputs $x\in[-0.99, 1]$, $y\in[1, 1.01]$,
$z\in[-0.01, 0.01]$.

All these above results are generated with the same type of floating-point
operators in each expression.  Although in this chapter we do not analyze
the number of \glspl{dsp} used in synthesized circuits, the \gls{dsp} count
increases linearly with the estimated \gls{lut} count.  In the next chapter
we further introduce the estimation of \gls{dsp} elements used as another
objective to optimize.

Because of the scalability problem of the depth limit $k$ mentioned in
Section~\ref{so:sec:equivalent}, $k \leq 3$ for all of our experiments.  By
setting $k = 4$, the tool does not terminate in reasonable amount of time and
saturates the memory (16 GB) of our system.  In the following chapters, we
propose methods to limit the number of iterations and the number of equivalent
expressions discovered to mitigate the lack of scalability of $k$.

Finally, Figure~\ref{so:fig:area} demonstrates the accuracy of the area
estimation used in our analysis. It compares the actual \glspl{lut} necessary
with the estimated number of \glspl{lut} using our semantics, by synthesizing
more than 6000 equivalent expressions derived from $\vara + \varb + \varc$,
$(\vara + 1) \times (\varb + 1) \times (\varc + 1)$, $e_1$, and $e_2$ using
varying mantissa widths. The dotted line indicates exact area estimation, a
scatter points that is close to the line means the area estimation for that
particular implementation is accurate. The solid black line represents the
linear regression line of all scatter points. On average, our area estimation
is a 6.1\% over-approximation of the actual number of \glspl{lut}, and the
worst case over-approximation is 7.7\%.
\newcommand{\figsize}{0.5}
\begin{figure}[ht]
    \centering
    \includegraphics[scale=\figsize]{martel}
    \caption{Optimization of ${(\vara + \varb)}^2$.}\label{so:fig:martel}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[scale=\figsize]{multi_expr_32}
    \caption{%
        Simultaneous optimization of both $e_1$ and $e_2$.
    }\label{so:fig:multi_expr_32}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[scale=\figsize]{multi_expr_vary_width}
    \caption{%
        Varying the mantissa width of Figure~\ref{so:fig:multi_expr_32}.
    }\label{so:fig:multi_expr_vary_width}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[scale=\figsize]{taylor_sin}
    \caption{The Taylor expansion of $\sin(x + y)$.}\label{so:fig:taylor_sin}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[scale=\figsize]{motzkin}
    \caption{The Motzkin polynomial $e_m$.}\label{so:fig:motzkin}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[scale=\figsize]{area}
    \caption{Accuracy of Area Estimation.}\label{so:fig:area}
\end{figure}
