\section{Related Work}
\label{sec:related_work}

High-level synthesis (HLS) is the process of compiling a high-level
representation of an application (usually in C, C++ or MATLAB) into
register-transfer-level (RTL) implementation for FPGA~\cite{coussy, gajski}.
HLS tools enable us to work in a high-level language, as opposed to facing
labor-intensive tasks such as optimizing timing, designing control logic in
the RTL implementation. This allows application designers to instead focus on
the algorithmic and functional aspects of their implementation~\cite{coussy}.
Another advantage of using HLS over traditional RTL tools is that a C
description is smaller than a traditional RTL description by a factor
of 10~\cite{coussy, bdti}, which means HLS tools are in general more
productive and less error-prone to work with. HLS tools benefit us in their
ability to automatically search the design space with a reasonable design
cost~\cite{bdti}, explore a large number of trade-offs between performance,
cost and power~\cite{mcfarland}, which is generally much more difficult to
achieve in RTL tools. HLS has received a resurgence of interest recently,
particularly in the FPGA community. Xilinx now incorporates a sophisticated
HLS flow into its Vivado design suite~\cite{vivado_hls} and the open-source
HLS tool, LegUp~\cite{legup}, is gaining significant traction in the research
community.

However, in both commercial and academic HLS tools, there is very little
support for static analysis of numerical algorithms. LLVM-based HLS
tools such as Vivado HLS and LegUp usually have some traditional static
analysis-based optimization passes such as constant propagation, alias
analysis, bitwidth reduction or even expression tree balancing to reduce
latency for numerical algorithms. There are also academic tools that
perform precision-performance trade-off by optimizing word-lengths of data
paths~\cite{constantinides}. However there are currently no HLS tools that
perform the trade-off optimization between accuracy and resource usage by
varying the \emph{structure} of arithmetic expressions.

Even in the software community, there are only a few existing techniques
for optimizing expressions by transformation, none of which consider
accuracy/run-time trade-offs. Darulova~\etal~\cite{darulova} employ a
metaheuristic technique. They use genetic programming to evolve the structure
of arithmetic expressions into more accurate forms. However there are several
disadvantages with metaheuristics, such as convergence can only be proved
empirically and scalability is difficult to control because there is no
definitive method to decide how long the algorithm must run until it reaches a
satisfactory goal. Hosangadi~\etal~\cite{hosangadi} propose an algorithm for
the factorization of polynomials to reduce addition and multiplication counts,
but this method is only suitable for factorization and it is not possible to
choose different optimization levels. Peymandoust~\etal~\cite{peymandoust}
present an approach that only deals with the factorization of polynomials
in HLS using Gr\"obner bases. The shortcomings of this are its dependence
on a set of library expressions~\cite{hosangadi} and the high computational
complexity of Gr\"obner bases. The method proposed by Martel~\cite{martel07}
is based on operational semantics with abstract interpretation, but even
their depth limited strategy is, in practice, at least exponentially complex.
Finally Ioualalen~\etal~\cite{ioualalen} introduce the abstract interpretation
of equivalent expressions, and creates a polynomially sized structure to
represent an exponential number of equivalent expressions related by rules of
equivalence. However it restricts itself to only a handful of these rules to
avoid combinatorial explosion of the structure and there are no options for
tuning its optimization level.

Since none of these above captures the optimization of both accuracy and
performance by restructuring arithmetic expressions, we base ourselves on the
software work of Martel~\cite{martel07}, but extend this work in the following
ways. Firstly, we develop new hardware-appropriate semantics to analyze not
only accuracy but also resource usage, seamlessly taking into account common
subexpression elimination. Secondly, because we consider both resource usage
and accuracy, we develop a novel multi-objective optimization approach to
scalably construct the Pareto frontier in a hierarchical manner, allowing fast
design exploration. Thirdly, equivalence finding is guided by prior knowledge
on the bounds of the expression variables, as well as local Pareto frontiers of
subexpressions while it is optimizing expression trees in a bottom-up approach,
which allows us to reduce the complexity of finding equivalent expressions
without sacrificing our ability to optimize expressions.

We begin with an introduction to formal semantics in the following section,
later in Section~\ref{sec:semantics}, we explain our approach by extending the
semantics to reason about errors, resource usage and equivalent expressions.
