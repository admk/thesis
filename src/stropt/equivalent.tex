\section{Equivalent Expressions Analysis}
\label{so:sec:equivalent}

In Section~\ref{bg:ssub:accuracy} of Chapter~\ref{chp:background}, we introduce
semantics that define additions and multiplications on intervals, then
gradually transition to error semantics that compute bounds of values and
errors.  In Section~\ref{so:sec:resource}, an alternative semantics that
eliminate common subexpressions are introduced, which we call the labelling
environments.  The environments define the meaning of arithmetic operations
on the environments.  In this section, we now take the leap from not only
analyzing an expression for its quality, to defining arithmetic operations on
sets of equivalent expressions, and use these rules to discover equivalent
expressions.  Before this, it is necessary to formally define equivalent
expressions and functions to discover them.


\subsection{Discovering Equivalent Expressions}
\label{so:sub:equivalent_relations}

From an expression, a set of equivalent expressions can be discovered by our
\emph{equivalence relation} $\eqrel$ on the set of all arithmetic expressions
$\aexprset$, and $\eqrel$ is a strict subset of $\aexprset\times\aexprset$.  It
is noteworthy that a relation is said to be an equivalence relation when it is
reflexive, symmetric and transitive, \ie~for all $e_1, e_2, e_3 \in \aexprset$,
we have the following rules in our inference system:
\begin{equation}
    \begin{aligned}{}
        & [\mathrm{Reflexivity}] & \quad &
            e_1 \eqrel e_1, \\
        & [\mathrm{Symmetry}] & \quad &
            \inference{e_1 \eqrel e_2}{e_2 \eqrel e_1}, \\
        & [\mathrm{Transitivity}] & \quad &
            \inference{e_1 \eqrel e_2 \wedge e_2 \eqrel e_3}{e_1 \eqrel e_3}.
    \end{aligned}
    \label{so:eq:equivalence_relation}
\end{equation}
Here, a rule of the form $\inference{a}{b}$ means that if the formula $a$ is
true, then $b$ also holds.  We extend our inference system with additional
arithmetic rules that relate equivalent expressions.  Let's define $e_1, e_2,
e_3 \in \aexprset$, $v_1, v_2, v_3 \in \realset$.  Firstly, we start with a set
of associativity rules:
\begin{equation}
    \begin{aligned}{}
        & [\mathrm{Assoc}_1] & \quad &
            \forall \opsymbol \in \{ +, \times \}:
            (e_1 \opsymbol e_2) \opsymbol e_3
            \eqrel e_1 \opsymbol (e_2 \opsymbol e_3), \\
        & [\mathrm{Assoc}_2] & \quad &
            \forall \opsymbol \in \{ +, \times, / \}:
            (e_1 \opsymbol e_2) \opsymbol e_3
            \eqrel (e_1 \opsymbol e_3) \opsymbol e_2, \\
        & [\mathrm{Assoc}_3 (/)] & \quad &
            (e_1 / e_2) / e_3 \eqrel e_1 / (e_2 \times e_3), \\
        & [\mathrm{Assoc}_4 (/)] & \quad &
            e_1 / (e_2 / e_3) \eqrel e_1 \times e_3 / e_2.
    \end{aligned}
\end{equation}
Secondly, we define commutativity rules for addition and multiplication:
\begin{equation}
    [\mathrm{Commut}] \quad
    \forall \opsymbol \in \{ +, \times \}:
        e_1 \opsymbol e_2 \eqrel e_2 \opsymbol e_1.
\end{equation}
Thirdly, distributivity rules enable further equivalent expressions to be
explored with our relation:
\begin{equation}
    \begin{aligned}{}
        & [\mathrm{Distrib}_1] & \quad &
            \forall \opsymbol \in \{ \times, / \}:
            e_1 \opsymbol e_3 + e_2 \opsymbol e_3
            \eqrel (e_1 + e_2) \opsymbol e_3, \\
        & [\mathrm{Distrib}_2 (\times)] & \quad &
            e_1 + e_1 \times e_2 \eqrel (1 + e_2) \times e_1, \\
        & [\mathrm{Distrib}_3 (\times)] & \quad &
            e_1 + e_1 \eqrel 2 \times e_1, \\
        & [\mathrm{Distrib}_4 (/)] & \quad &
            e_1 + e_2 / e_3 \eqrel (e_1 \times e_3 + e_2) / e_3, \\
        & [\mathrm{Distrib}_5 (/)] & \quad &
            e_1 / e_3 + e_2 / e_4 \eqrel
            (e_1 \times e_4 + e_2 \times e_3) / (e_3 \times e_4), \\
        & [\mathrm{Distrib}_6 (-)] & \quad &
            \forall \opsymbol \in \{ +, - \}:
            -(e_1 \opsymbol e_2) \eqrel (-e_1) \opsymbol (-e_2), \\
        & [\mathrm{Distrib}_7 (-)] & \quad &
            \forall \opsymbol \in \{ \times, / \}:
            -(e_1 \opsymbol e_2)
            \eqrel (-e_1) \opsymbol e_2 \eqrel e_1 \opsymbol (-e_2).
    \end{aligned}
\end{equation}
The following rule rewrites an expression with subtraction into an addition:
\begin{equation}
    [\mathrm{Subtract}] \quad
    e_1 - e_2 \eqrel e_1 + (-e_2).
\end{equation}
The following reduction rules propagates constant values in expression
trees. The $\mathrm{ConstProp}_1$ rule states that if an expression is an
arithmetic operation of two constant values, then it can be simply evaluated to
produce the result; and $\mathrm{ConstProp}_2$ rewrites an expression that has
a negation on a constant value into a single constant.
\begin{equation}
    \begin{aligned}{}
        & [\mathrm{ConstProp_1}] &~&
        \inference{%
            (v_3 = v_1 \opsymbol v_2) \wedge
            (\opsymbol \in \binaopset)
        }{%
            v_1 \opsymbol v_2 \eqrel v_3
        }, \\
        & [\mathrm{ConstProp_2} (-)] &~&
        \inference{%
            v_2 = -v_1
        }{%
            -v_1 \eqrel v_2
        }.
    \end{aligned}
    \label{so:eq:constant_propagation}
\end{equation}
Here we define $\binaopset = \{ +, -, \times, / \}$ to be the set of binary
arithmetic operators.  We also define another set of reduction rules looking
for common patterns that can be simplified:
\begin{equation}
    \begin{aligned}{}
        & [\mathrm{Identity}_1] &~&
            \forall \opsymbol \in \{ \times, / \}:
            e_1 \opsymbol 1 \eqrel e_1,
            & \quad
        & [\mathrm{Elim}_1 (-)] &~&
            e_1 - e_1 \eqrel 0,
            \\
        & [\mathrm{Identity}_2] &~&
            \forall \opsymbol \in \{ +, - \}:
            e_1 \opsymbol 0 \eqrel e_1,
            & \quad
        & [\mathrm{Elim}_2 (/)] &~&
            e_1 / e_1 \eqrel 1,
            \\
        & [\mathrm{ZeroProp} (\times)] &~&
            0 \times e_1 \eqrel 0,
            & \quad
        & [\mathrm{DoubleNeg} (-)] &~&
            -(-e_1) \eqrel e_1.
            \\
    \end{aligned}
    \label{so:eq:common_reduction}
\end{equation}
Finally, the following two allow structural induction on expression trees,
\eg~it is possible to derive that $a + (b + c) \eqrel a + (c + b)$ from $b + c
\eqrel c + b$:
\begin{equation}
    \begin{aligned}
        & [\mathrm{Tree}_1] &~&
        \inference{%
            \left( e_1 \eqrel e_2 \right) \wedge
            \left( e_3 \eqrel e_4 \right) \wedge
            \left( \opsymbol \in \binaopset \right)
        }{%
            e_1 \opsymbol e_3 \eqrel e_2 \opsymbol e_4
        }, \\
        & [\mathrm{Tree}_2] &~&
        \inference{%
            e_1 \eqrel e_2
        }{%
            -e_1 \eqrel -e_2
        }.
    \end{aligned}
    \label{so:eq:equivalence_tree}
\end{equation}

We say that $e_1$ is equivalent to $e_2$ if and only if $e_1 \eqrel e_2$ For
some expressions $e_1$ and $e_2$.  Many rules are considered redundant and thus
excluded from our equivalence relation, as these can be derived by combining
rules from our inference system.  For instance, $e_1 \times (e_2 + e_3) \eqrel
e_1 \times e_2 + e_1 \times e_3$ can be derived by using $\mathrm{Distrib}_1$,
in tandem with the $\mathrm{Commut}$ rule.


\subsection{Scalable Methods for Rewriting}
\label{so:sub:scalable}

The above rules of equivalence relate an expression with all of its equivalent
expressions.  In general because of combinatorial explosion, the set of all
equivalent expressions is so large to be derived, which motivates us to develop
scalable methods that execute fast enough even with large expressions.

Instead of deriving the full set of equivalent expressions, we can define
a new relation $\eqgenrel$, a subset of $\eqrel$, which is identical to
our equivalent relation $\eqrel$ except that we place a few restrictions
on the relation.  This new relation can be generated by removing the
equivalence relation rules in~\eqref{so:eq:equivalence_relation}.  Firstly,
reflexivity can be removed, because it is not necessary to rediscover
expressions.  Secondly, we disable transitivity from $\eqrel$, as we
can have the flexibility to apply $\eqgenrel$ in a series of steps
to generate equivalent expressions.  Finally, we further disallow
symmetry in~\eqref{so:eq:equivalence_relation} for the reduction rules
in~\eqref{so:eq:constant_propagation}~and~\eqref{so:eq:common_reduction} to
reduce the space-time complexity of the search space, because often performance
metrics, such as accuracy and resource usage, improve when the number of terms
in the expression is reduced.

To make use of the new relation we define the following category of functions:
\begin{definition}
    We call a function an \gls{eeg} function if and only if the function takes
    as an input an initial set of equivalent expressions, and generates a set
    of equivalent expressions.
\end{definition}

For instance, an \gls{eeg} function $\eqstep: \eqexprpowerset \to
\eqexprpowerset$, where $\eqexprpowerset$ denotes the power set of all
equivalent expressions $\eqexprset$, can be defined as follows:
\begin{equation}
    \eqstep(\epsilon) = \left\{
        e^\prime\in\aexprset \mid
        e \eqgenrel e^\prime \wedge e\in\epsilon\right\},
    \label{so:eq:eqstep}
\end{equation}
where $\epsilon$ is a set of equivalent expressions.

We define a functional:
\begin{equation}
    \closure_N:
        \left( \eqexprpowerset \to \eqexprpowerset \right) \to
        \left( \eqexprpowerset \to \eqexprpowerset \right),
\end{equation}
which takes as an input a \gls{eeg} function and produces another \gls{eeg}
function:
\begin{equation}
    f^\prime (\epsilon) \defeq
        \bigcup_{i = 0}^N f^i(\epsilon),
    \text{~where~} f^\prime = \closure_N (f),
    \label{so:eq:transitive_generator}
\end{equation}
here, $f$ and $f^\prime$, respectively the input and output of $\closure_N$,
are both \gls{eeg} functions.  In the rest of the section, we omit the brackets
surrounding the input of $\closure_N$ for simplicity, \eg~$\closure_N (f)$ can
be written as $\closure_N f$.

As an example use of the functional $\closure_N$, we may note that we can
substitute $f$ with $\eqstep$ in $\closure_N f (\epsilon)$ to generate a set
of equivalent expressions, by taking the union of $N$ steps of $\eqstep$ of
$\epsilon$.  By further allowing $N$ to approach $\infty$, we obtain the full
set of equivalent expressions of $\epsilon$ that can be discovered using our
inference system, \ie~the transitive closure of equivalent expressions related
by $f$, from an initial set of equivalent expressions $\epsilon$:
\begin{equation}
    \closure_\infty \eqstep (\epsilon) =
        \bigcup_{i = 0}^\infty \eqstep^i(\epsilon).
    \label{so:eq:transitive_closure}
\end{equation}
Alternatively, we can view $\closure_\infty f$ as computing the
least fixpoint of $g$:
\begin{equation}
    \lfp g = \bigcup_{i = 0}^\infty g^i(\varnothing),
    \quad \text{where~} g(\varepsilon) \defeq f(\varepsilon) \cup \epsilon.
\end{equation}
We may further omit the $\infty$ from $\closure_\infty$ to denote the
transitive closure, \eg~the above example in~\eqref{so:eq:transitive_closure}
can be simplified to be $\closure \eqstep (\epsilon)$.

In practice, it is often infeasible to generate the full transitive closure of
a given expression, we therefore impose further constraints on how we discover
equivalent expressions.

Firstly, instead of exploring the full transitive closure, that is, by allowing
the number of steps $N$ in~\eqref{so:eq:transitive_generator} to be infinite,
we may restrict $N$ to be a small finite value to allow a smaller set of
equivalent expressions to be computed.

Secondly, the complexity of equivalent expression finding is reduced by
fixing the structure of subexpressions at a certain depth $k$ in the original
expression.  The definition of depth is given as follows: first the root of
the parse tree of an expression is assigned depth $d = 1$; then we recursively
define the depth of a node as one more than the depth of its greatest-depth
parent.  If the depth of the node is greater than $k$, then we fix the
structure of its child nodes by disallowing any equivalence transformation
beyond this node. We let $\eqstep_k$ denote this ``depth-limited'' equivalence
finding function, where $k$ is the depth limit used.  We can then use
$\closure_N \eqstep_k$ and $\closure \eqstep_k$ to denote the functions to
respectively compute the union of $N$ steps of $\eqstep_k$ and the transitive
closure. This approach is similar to Martel's depth-limited equivalent
expression transform~\cite{martel07}, however Martel's method eventually allows
transformation of subexpressions beyond the depth limit, because rules of
equivalence would transform these to have a smaller depth.  This contributes
to a time complexity at least exponential in terms of the expression size. In
contrast, our technique has a time complexity that does not depend on the size
of the input expression, but grows with respect to the depth limit $k$. Note
that the full equivalence closure using the inference system we defined earlier
in~\eqref{so:eq:transitive_closure} is at least $\bigo((2n - 3)\,!!)$ where $n$
is the number of terms in an expression, as we discussed earlier.

Finally, we use an iterative algorithm to accelerate the computation of
$\closure_N \eqstep_k (\epsilon)$.  In each iteration, we keep track
of the equivalent expressions that are newly discovered in the current
iteration, so that in the next iteration we apply $f$ only to those
expressions, to avoid redundant computation.  This algorithm is shown in
Figure~\ref{so:alg:closure} to efficiently compute $\closure_N f(\epsilon)$,
where $f$ can be $\eqstep_k$.  The correctness of this algorithm is discussed
in greater depth in Appendix~\ref{app:sound_acceleration}.
\begin{figure}[ht]
    \centering
    \begin{algorithmic}
        \singlespacing%
        \Function{Closure}{$f$, $N$, $\epsilon$}
            \State{$s_0 \gets \epsilon$}
            \State{$s^\prime_0 \gets \epsilon$}
            \For{$i \gets 1, \ldots, N$}
                \State{$s^\prime_i \gets
                    f \left(s^\prime_{i-1}\right) - s_{i-1}$}
                \State{$s_i \gets s_{i-1} \cup s^\prime_i$}
                \If{$s^\prime_i = \emptyset$}
                    \State{\Return{$s_i$}}
                \EndIf{}
            \EndFor{}
            \State{\Return{$s_i$}}
        \EndFunction{}
    \end{algorithmic}
    \caption{%
        Our algorithm to compute $\closure_N f (\epsilon)$, which discovers a
        set of equivalent expressions with a $\cup$-distributive \acrshort{eeg}
        $f$ from an initial set of equivalent expressions $\epsilon$.
    }\label{so:alg:closure}
\end{figure}


\subsection{Pareto Frontier}
\label{so:sub:pareto}

Because we optimize expressions in two quality metrics, \ie~the accuracy of
computation and the estimate of \gls{fpga} resource utilization, there is
a trade-off between them. We desire the largest subset of all equivalent
expressions $E$ discovered such that in this subset, no expression dominates
any other expression, in terms of having both better area and better
accuracy. This subset is known as the \emph{Pareto frontier}~\cite{legriel10}.

Figure~\ref{so:alg:pareto} shows a simplified algorithm for calculating
the Pareto frontier for a set of equivalent expressions $\epsilon$, faster
algorithms exist.  For instance, we could sort these expressions by accuracy,
as a fast sorting algorithm takes $\bigo(n\log(n))$ for $n$ elements, and then
prune suboptimal ones in one pass.
\begin{figure}[ht]
    \centering
    \begin{algorithmic}
        \singlespacing%
        \Function{Frontier}{$\epsilon$}
            \State{$\mathit{frontier} \gets \epsilon$}
            \For{$e \in \epsilon$}
                \For{$e^\prime \in \epsilon$}
                    \If{$\mathit{Area}(e^\prime) < \mathit{Area}(e)$ and
                        $\abserr(e^\prime) < \abserr(e)$}
                        \State{%
                            $\mathit{frontier} \gets
                                \mathit{frontier} / \{ e \}$}
                    \EndIf{}
                \EndFor{}
            \EndFor{}
            \State{\Return{$\mathit{frontier}$}}
        \EndFunction%
    \end{algorithmic}
    \caption{%
        The algorithm used to compute $\frontier(\epsilon)$, \ie~the Pareto
        frontier from a set of equivalent expressions $\epsilon$.
    }\label{so:alg:pareto}
\end{figure} \\
Here, $\mathit{frontier} / \{ e \}$ is a set identical to $\mathit{frontier}$,
except that the element $e$ is removed.  We use the function $\abserr$ to
analyze the magnitudes of error bounds as defined in~\eqref{so:eq:abserr}.


\subsection{Equivalent Expressions Semantics}
\label{so:sub:equivalent_semantics}

Similar to the analysis of accuracy and resource usage, a set of equivalent
expressions can be computed with semantics. That is, we define structures,
\ie~sets of equivalent expressions, that can be manipulated with arithmetic
operators. In our equivalent expressions semantics, an element of
$\eqexprpowerset$ is used to assign a set of expressions to each node
in an expression parse tree. To begin with, at each leaf of the tree, the
variable or constant is assigned a set containing itself, as for $x$, the set
$\epsilon_x$ of equivalent expressions is $\epsilon_x = \{x\}$. After this, we
propagate the equivalence expressions in the parse tree's direction of flow,
using~\eqref{so:eq:equivalence_semantics} defined below:
\begin{equation}
    \begin{aligned}
        \epsilon_x \opsymbol \epsilon_y &\defeq \frontier\left(
            \closure \eqstep_k \left(
                E_\opsymbol \left( \epsilon_x, \epsilon_y \right)
            \right) \right), \\
        & \text{where~}
        E_\opsymbol(\epsilon_x, \epsilon_y) = \left\{
            e_x \opsymbol e_y \mid e_x \in \epsilon_x \wedge e_y \in \epsilon_y
        \right\}, \\
        & \text{and~} \opsymbol\in\{ +, -, \times, / \}.
    \end{aligned}
    \label{so:eq:equivalence_semantics}
\end{equation}
It is noteworthy that we override the meaning of $\opsymbol$, from arithmetic
computations originally, to denote the construction of equivalent expressions.
The equation implies that in the propagation procedure, it recursively
constructs a set of equivalent subexpressions for the parent node from two
child expressions, and uses the depth-limited equivalence function $\closure
\eqstep_k$ to work out a larger set of equivalent expressions.  Similarly, we
can define another equation that propagate equivalent subexpressions in an
expression with a unary subtraction:
\begin{equation}
    \begin{aligned}
        -\epsilon &\defeq \frontier\left(
            \closure \eqstep_k \left(
                E_{\mathrm{unary}} \left( \epsilon \right)
            \right) \right), \\
        & \text{where~}
        E_\mathrm{unary} \left( \epsilon \right) = \left\{
            -e \mid e \in \epsilon
        \right\}.
    \end{aligned}
    \label{so:eq:equivalence_semantics_unary}
\end{equation}

To reduce computation effort, we select only those expressions on the Pareto
frontier for the propagation in the \gls{dfg}\@. Although in worst case the
complexity of this process is exponential, the selection by Pareto optimality
accelerates the algorithm significantly. For example, for the subexpression $a
+ b$ of our sample expression in Figure~\ref{so:fig:sample_tree}:
\begin{equation}
    \begin{aligned}
        \epsilon_a + \epsilon_b
            &= \frontier\left(
                    \closure \eqstep_k \left(
                        E_\opsymbol \left( \epsilon_a, \epsilon_b \right)
                    \right)
                \right) \\
            &= \frontier\left(
                    \closure \eqstep_k \left(
                        E_\opsymbol \left( \{a\}, \{b\} \right)
                    \right)
                \right) \\
            &= \frontier\left(
                    \{ a + b, b + a \}
                \right).
    \end{aligned}
\end{equation}

Alternatively, we could view the semantics in terms of \glspl{dfg} representing
the algorithm for finding equivalent expressions. The parsing of an expression
directly determines the structure of its \gls{dfg}\@. For instance, consider
the tree structure of the expression $e_0 = (a + b) \times (a + b)$, as shown
in Figure~\ref{so:fig:sample_tree}. This tree structure can be used to generate
a \gls{dfg} illustrated in Figure~\ref{so:fig:tree_expr_flow}, which when
data-flow analysis is applied, discovers a set of equivalent expressions to
$e_0$. The circles labeled $3$ and $7$ in this diagram are shorthands for the
operation $E_+$ and $E_\times$ respectively, where $E_+$ and $E_\times$ is
defined in~\eqref{so:eq:equivalence_semantics}.
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{tree_expr_flow}
    \caption{%
        The \acrshort{dfg} for finding equivalent expressions of
        $(a + b) \times (a + b)$.
    }\label{so:fig:tree_expr_flow}
\end{figure}

For our example in Figure~\ref{so:fig:tree_expr_flow},
similar to the construction of data-flow equations in
Section~\ref{bg:sec:abstract_interpretation} of Chapter~\ref{chp:background},
we can produce a set of equations from the data-flow of the \gls{dfg}, which
now produces equivalent expressions:
\begin{equation}
    \begin{aligned}
        \enter{1} &= \enter{1} \cup \{a\}, &
        \enter{2} &= \enter{2} \cup \{b\}, \\
        \enter{3} &= E_+(\enter{1}, \enter{2}), &
        \enter{4} &= \enter{3} \cup \enter{5}, \\
        \enter{5} &= \eqstep_k(\enter{4}), &
        \enter{6} &= \frontier(\enter{5}), \\
        \enter{7} &= E_\times(\enter{6}, \enter{6}), &
        \enter{8} &= \enter{7} \cup \enter{9}, \\
        \enter{9} &= \eqstep_k(\enter{8}), &
        \enter{10} &= \frontier(\enter{9}).
    \end{aligned}
    \label{so:eq:tree_expr_flow}
\end{equation}
Because of loops in the \gls{dfg}, it is no longer trivial to find the
solution.  In general, the analysis equations are solved iteratively, using
the \gls{dfa} approach discussed in Section~\ref{bg:sub:data_flow} of
Chapter~\ref{chp:background}. We can regard the set of equations as a single
transfer function $F$ as in \eqref{so:eq:transfer_function}, where the function
$F$ takes as input the variables $A(1), \ldots, A(10)$ appearing in the
right-hand sides of \eqref{so:eq:tree_expr_flow} and outputs the values $A(1),
\ldots, A(10)$ appearing in the left-hand sides. Our aim is then to find an
input $\vec{x}$ to $F$ such that $F(\vec{x}) = \vec{x}$, \ie~a fixpoint of $F$.
\begin{equation}
      F((\enter{1}, \ldots, \enter{10}))
    = (\enter{1} \cup \{a\}, \ldots, \frontier(\enter{9})).
    \label{so:eq:transfer_function}
\end{equation}
Initially we assign $\enter{i} = \varnothing$ for $i\in\{1,2,\ldots,10\}$, and
we denote $\vec\varnothing = (\varnothing, \ldots, \varnothing)$.  Then we
compute the least fixpoint of $F$:
\begin{equation}
    \lfp F \defeq \bigcup_{n \in \realset} F^n(\vec\varnothing).
\end{equation}
This expression can be computed iteratively by first evaluating
$F(\vec\varnothing)$, $F^2(\vec\varnothing) = F(F(\vec\varnothing))$,
and so forth, until the fixpoint is reached for some iteration $n$,
\ie~$F(F^n(\vec\varnothing)) = F^{n + 1}(\vec\varnothing)$. Hence we know that
for any iterations $m > n + 1$, $F^m(\vec\varnothing) = F^n(\vec\varnothing)$.
The value $n$ should be a finite constant, because the relation $\eqgenrel$
can only reach a finite number of expressions.  In cases when $\lfp F$ is
computational intensive, we could limit the number of iterations $n$, to
compute an under-approximation (a subset) of $\lfp F$.

The fixpoint solution $\lfp F$ gives a set of equivalent expressions derived
using our method, which is found at $\enter{10}$. In essence, the depth limit
acts as a sliding window.  The semantics allow hierarchical transformation of
subexpressions using a depth-limited search and the propagation of a set of
subexpressions that are locally Pareto optimal to the parent expressions in a
bottom-up hierarchy.

The problem with the semantics above is that the time complexity of $\closure
\eqstep_k$ scales poorly, since the worst case number of subexpressions needed
to explore increases exponentially with $k$. Therefore an alternative method
is to optimize it by changing the structure of the \gls{dfg} slightly, as
shown in Figure~\ref{so:fig:tree_expr_flow_greedy}.  The difference is that
at each iteration, the Pareto frontier filters the results to decrease the
number of expressions to process for the next iteration.  Equivalently, this
approach yields the following semantics for arithmetic operations on equivalent
expressions as an alternative to~\eqref{so:eq:equivalence_semantics}:
\begin{equation}
    \begin{aligned}
        \epsilon_x \opsymbol \epsilon_y &\defeq
            \closure \left( \frontier \circ \eqstep_k \right) \left(
                E_\opsymbol \left( \epsilon_x, \epsilon_y \right)
            \right), \quad\text{where~} \opsymbol\in\{ +, -, \times, / \}, \\
        -\epsilon &\defeq
            \closure \left( \frontier \circ \eqstep_k \right) \left(
                E_{\mathrm{unary}} \left( \epsilon \right)
            \right).
    \end{aligned}
    \label{so:eq:equivalence_semantics_alt}
\end{equation}
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{tree_expr_flow_greedy}
    \caption{%
        The alternative \acrshort{dfg} for $(a + b) \times (a + b)$.
    }\label{so:fig:tree_expr_flow_greedy}
\end{figure}

In the rest of this chapter, we use \frontiertrace{} to indicate our equivalent
expression finding semantics, and \greedytrace{} to represent the alternative
method.

In addition to the above approaches, another possibility is to view the
optimization in a perspective from denotational semantics.  We can define
a recursively-defined function $\optfunc{e}\sigma^\sharp$ which accepts an
expression $e$, and $\sigma^\sharp$, an input condition on the variables.  This
function produces a set of optimized expressions equivalent to $e$.  The set
of input conditions will be formally defined in Chapter~\ref{chp:progopt}.
Therefore for $e_1, e_2 \in \aexprset$ and a variable $\varx$:
\begin{equation}
    \begin{aligned}
        \optfunc{e_1 \opsymbol e_2}\sigma^\sharp &=
        f_{\sigma^\sharp} \left(
            \left\{
                e^\prime_1 \opsymbol e^\prime_2 \mid
                e^\prime_1 \in \optfunc{e_1}\sigma^\sharp,
                e^\prime_2 \in \optfunc{e_2}\sigma^\sharp
            \right\}
        \right), \\
        \optfunc{\varx}\sigma^\sharp &= \{\varx\},
    \end{aligned}
\end{equation}
where $f_\sigma^\sharp$ is a function that transforms and optimizes a set
of equivalent expressions based on the initial condition $\sigma^\sharp$,
\eg~$\frontier \circ \closure \eqstep_k$ or $\closure \left(\frontier \circ
\eqstep_k\right)$ used in \frontiertrace{} or \greedytrace{}.


\subsection{Simultaneous Optimization of Multiple Expressions}
\label{so:sub:multiple_expressions}

To simultaneously optimize multiple expressions, a new operator, the barrier
operator ``$\mid$'', is introduced to concatenate expressions.  Multiple
expressions, when concatenated, allow common subexpressions to be shared.  For
instance, the expressions $a + (b + c)$ and $a \times (b + c)$ can concatenate
to form a new expression $e$:
\begin{equation}
    e = a + (b + c) \mid a \times (b + c),
\end{equation}
and the subexpression $b + c$ is shared within $e$, as this behaviour
naturally arises from the resource usage analysis discussed in
Section~\ref{so:sec:resource}.

The barrier operator has no rules of equivalence to discourage any transforms
across the expression boundaries.  For a single expression, the quality of
accuracy is determined by its round-off error.  However when evaluating
multiple expressions there are choices to make in terms of determining the
overall accuracy of the system of multiple expressions.  The reasons are
two-fold~\cite{martel09}.  Firstly, perhaps only a user-defined subset of the
expressions that compute the final results are influential.  Secondly, we may
wish to minimize either the $L^1$-, $L^2$-, $L^\infty$-norm, or a geometric
mean of the errors, depending on which one is more relevant.  For this reason,
\soap{} provides the choice to minimize any of these above norms, and is
designed to minimize the $L^\infty$-norm of all expressions by default, by
computing the least upper bound of the following accuracy semantics of two
expressions joined by the barrier operator:
\begin{equation}
    (x^\sharp_1, \mu^\sharp_1) \mid (x^\sharp_2, \mu^\sharp_2)
    \defeq (x^\sharp_1, \mu^\sharp_1) \join (x^\sharp_2, \mu^\sharp_2),
\end{equation}
where the $\join$ operator on the abstract semantics is introduced in
Section~\ref{bg:ssub:accuracy}.

With the extended accuracy analysis in place, the automatic multi-objective
optimization techniques for single expression can be adapted to multiple
expressions, by concatenating them into a single expression using the barrier
operator.
