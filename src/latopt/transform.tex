\section{Structural Optimization}
\label{lo:sec:structural_optimization}

From a numerical program, we can generate a \gls{mir} using the translation
process in Section~\ref{lo:sec:intermediate}.  The next step is to transform
the \gls{mir}, and discover \glspl{mir} that are equivalent to the original
\gls{mir} in real arithmetic, but may execute differently in finite-precision
arithmetic because of round-off errors.

\subsection{Improved Algorithm}
\label{lo:sub:algorithm}

% Our optimization starts by partitioning the \gls{mir} into sub-\glspl{mir}.
% This further reduces the size of the search space, \ie~equivalent \glspl{mir}
% reachable using our transformation rules.  This speeds up the rest of the
% optimization process, because these rules are not applied on partition
% boundaries.

As discussed in previous chapters, even a small expression could have a huge
number of equivalent ones.  Exhaustively discovering all equivalent \glspl{mir}
would result in combinatorial explosion of the number of equivalent \glspl{mir}
in the search space.  For this reason, we base ourselves on an algorithm from
Section~\ref{po:sec:equivalence_analysis} of Chapter~\ref{chp:progopt} that
searches efficiently by discovering equivalences in a bottom-up hierarchy.  In
this section, we discuss two major improvements to the algorithm which further
increase its performance.

\subsubsection{Partitioning}

Instead of optimizing the \gls{mir} immediately, we start by partitioning the
\gls{mir} into multiple smaller sub-\glspl{mir}.  The partition boundaries are
determined by $\updateop$ operators.  For instance, we consider the partially
unrolled Fibonacci example in Figure~\ref{lo:fig:fib2}.  The \gls{mir} of the
loop body is shown in Figure~\ref{lo:fig:fib_mir2}.  The partition boundaries
are indicated by the region surrounded by the red dotted curve \tikz{\draw[red,
dashed, rounded corners=1ex] (0,0) rectangle (20pt,2ex);}.  A multiply shared
subexpression, such as $\texttt{i} - 1$, also determines the partition boundary
by merging its partition with one of its parents with the smallest partition by
node count.  If all parents contain the same number of nodes then a choice is
made randomly.
\begin{figure}[ht]
    \centering
    \newsavebox{\fiblstb}
    \begin{lrbox}{\fiblstb}
        \begin{lstlisting}
for (int i=3; i<1023; i+=2)
{
    A[i-1] = A[i-2]+A[i-3];
    A[i] = A[i-1]+A[i-2];
}
        \end{lstlisting}
    \end{lrbox}
    \subfloat[The partially unrolled loop.]{%
        \begin{minipage}{0.45\textwidth}
            \usebox{\fiblstb}
        \end{minipage}\label{lo:fig:fib2}
    }
    \subfloat[The \gls{mir} of the partially unrolled loop body.]{%
        \begin{tikzpicture}[mir]
            \node[mirnode] (mA) at (0, 0) {\texttt{A}};
            \node[mirnode] (upd) [right=of mA] {$\updateop$};
            \node[mirnode] (updi) [below=of upd] {\texttt{i}};
            \node[mirnode] (+) [below right=of upd] {$+$};
            \node[mirnode] (acc1) [below left=of +] {$\accessop$};
            \node[mirnode] (acc2) [below right=of +] {$\accessop$};
            \node[mirnode] (-1) [below right=6mm of acc1] {$-$};
            \node[mirnode] (1) [below right=of -1] {$1$};
            \node[mirnode] (-1i) [below left=of -1] {\texttt{i}};
            \draw[<-] (upd) -- (+);
            \draw[<-] (+) -- (acc1);
            \draw[<-] (+) -- (acc2);
            \draw[<-] (acc1) -- (-1);
            \draw[<-] (-1) -- (1);
            \draw[<-] (upd) -- (updi);
            \draw[<-] (-1) -- (-1i);

            \node[mirnode] (2upd) [below left=8mm of acc1] {$\updateop$};
            \node[mirnode] (2+) [below right=8mm of 2upd] {$+$};
            \node[mirnode] (2acc1) [below left=of 2+] {$\accessop$};
            \node[mirnode] (2acc2) [below right=of 2+] {$\accessop$};
            \node[mirnode] (-3) [below right=6mm of 2acc1] {$-$};
            \node[mirnode] (3) [below right=of -3] {$3$};
            \node[mirnode] (-3i) [below left=of -3] {\texttt{i}};
            \node[mirnode] (Ain) [below left=8mm of 2acc1] {\texttt{A}};
            \draw[<-] (2upd) -- (2+);
            \draw[<-] (2+) -- (2acc1);
            \draw[<-] (2+) -- (2acc2);
            \draw[<-] (2acc1) -- (-3);
            \draw[<-] (2acc1) -- (Ain);
            \draw[<-] (-3) -- (3);
            \draw[<-] (2upd) -- (Ain);
            \draw[<-, channel] (2upd) to[out=-70, in=180] (-1);
            \draw[<-, channel] (2acc2) -- (Ain);
            \draw[<-] (-3) -- (-3i);

            \node[mirnode] (-2) [below right=6mm of acc2, yshift=-5mm] {$-$};
            \node[mirnode] (2) [below right=of -2] {$2$};
            \node[mirnode] (-2i) [below left=of -2] {\texttt{i}};
            \draw[<-] (2acc2) to[out=0, in=180] (-2);
            \draw[<-] (-2) -- (-2i);
            \draw[<-] (-2) -- (2);
            \draw[<-] (acc2) -- (-2);

            \draw[<-] (upd) -- (2upd);
            \draw[<-] (acc1) -- (2upd);
            \draw[<-, channel] (acc2) -- (2upd);

            \node[mirnode] (mi) [right=15mm of upd] {\texttt{i}};
            \node[mirnode] (i+2) [right=of mi] {$+$};
            \node[mirnode] (i+2 i) [below left=of i+2] {\texttt{i}};
            \node[mirnode] (i+2 2) [below right=of i+2] {$2$};
            \draw[|->] (mA) -- (upd);
            \draw[|->] (mi) -- (i+2);
            \draw[<-] (i+2) -- (i+2 i);
            \draw[<-] (i+2) -- (i+2 2);

            \draw[red, dashed, rounded corners]
                (3.south east) -- (-3i.south) --
                (Ain.south west) -- (Ain.north west) --
                (2upd.north west) -- (2upd.north east) --
                (2acc2.north east) -- cycle;
            \draw[red, dashed, rounded corners]
                (2.north east) -- (2.south east) -- (-2i.south west) --
                (-1i.south west) -- (acc1.south west) --
                (upd.north west) -- (upd.north east) --
                (acc2.north east) -- cycle;
            \draw[red, dashed, rounded corners]
                (i+2.north) -- (i+2 i.north west) -- (i+2 i.south west) --
                (i+2 2.south east) -- (i+2 2.north east) -- cycle;
            \brackets{(current bounding box)};
        \end{tikzpicture}
        {}\label{lo:fig:fib_mir2}
    }
    \caption{An example to illustrate how \acrshortpl{mir} are partitioned.}
\end{figure}

Because transformation rules can only be applied to each partition but not
across them, the size of the search space can be reduced further.  In turn,
each are optimized separately and generate a set of partitions equivalent to
the original.  We then select combinations from these partitions to be merged,
this generates a set of \glspl{mir} that are equivalent to the original.
Finally, we preserve those \glspl{mir} merged on the Pareto frontier.

\subsubsection{Optimization}

% Figure~\ref{lo:alg:optimize} shows the pseudocode of the optimization
% algorithm.  It takes as an input a \gls{mir} graph, and produces a set of
% equivalent graphs that are estimated to be Pareto-optimal when converted into
% C programs and synthesized into circuits.  Although this algorithm deals with
% a special case, \ie~a root node $op$ with two child subtrees $e_1, e_2$, it
% can easily be generalized to an arbitrary number of child subtrees.  Here,
% $e \stackrel{r}{\eqgenrel} e'$ means $e^\prime$ can be obtained by
% transforming part of the graph $e$ in accordance with the transformation rule
% $r$.  The next section discusses the transformation rules we used.

% The algorithm starts by discovering equivalences in the leaves of a
% \gls{mir}, and progresses upwards for equivalent structures of the individual
% components that make up the graph, until the roots of the graph, where we
% have a set of \glspl{mir} equivalent to the original \gls{mir}\@.  As it
% traverses through the \gls{mir}, the algorithm calculates the performance
% metrics at each node, using the analyses presented in the next section.
% Transformations that are not Pareto-optimal are immediately pruned from the
% search space, thus reducing the average complexity of the algorithm.

Previously in \soap, as we optimize parts of \glspl{mir}, the Pareto
frontier is used to filter discovered equivalent candidates (\glspl{mir} or
semantic expressions), which keeps the size of the set relatively small and
manageable.  As we optimize larger programs, the run time of the tool increases
significantly.  Currently, the \soap{} framework prunes the \glspl{mir} that
are Pareto-suboptimal, leaving only those that are on the Pareto frontier.
However, because our Pareto frontier is three-dimensional, there is a large
increase in the number of Pareto-optimal \glspl{mir}.  This Pareto pruning
approach is no longer feasible for our benchmark examples.  Therefore in this
chapter, not only do we use the Pareto frontier to filter candidates, we also
introduce a \textsc{Prune} function to further reduce the size of Pareto
frontier.

We rely on the \textsc{Prune} function to efficiently steer the direction
of our Pareto frontier as we discover new candidates.  It takes as an
input the set of Pareto-optimal equivalent candidates that we have
discovered, and prunes elements in this set to reduce its size by sampling,
keeping the number of discovered \glspl{mir} tractable.  The pruning
algorithm is inspired by Poisson-disk sampling algorithm~\cite{bridson07}.
Our algorithm in Figure~\ref{lo:alg:sample} starts by first randomly
selecting one point from the Pareto frontier $\epsilon_0$ (denoted by
$\textsc{RandomSample}(\epsilon_0)$).  It then grows the set of points by
adding the neighbours from the point that are separated by at least a certain
distance $\delta$, where the distance $\delta$ is decreased iteratively by
a factor $\zeta = 0.8$, until $\epsilon$ contains at least $\eta = 20\%$
of all points in the Pareto frontier, or a maximum number of attempts
($\mathrm{attempt\_count}$) is reached.  The distance between two options $e$
and $e^\prime$ is computed as follows:
\begin{equation}
    \mathrm{dist} \left(e, e^\prime\right)
    = \sqrt{
        \sum_{f \in F} {\left(
            \frac{f(e) - f(e^\prime)}{\max(f(e), f(e^\prime))}
        \right)}^2
    },
\end{equation}
where $f \in F$ enumerates each function that evaluates the performance of a
candidate $e$, \ie~the function $f \in F$ computes either the accuracy, area
or latency of $e$.

\begin{figure}[ht]
    \centering
    \begin{algorithmic}
        \singlespacing%
        \Function{Sample}{$\epsilon_0$}
            \State{$\delta = 1.0$}
            \State{%
                $\epsilon = \left\{
                    \textsc{RandomSample}\left(\epsilon_0\right)
                \right\}$}
            \For{%
                $i = 1, 2, \mathellipsis, \mathrm{attempt\_count}$
            }
                \For{$e \in \epsilon_0$}
                    \State{$\mathrm{in\_range} = \falselit$}
                    \For{$e^\prime \in \epsilon$}
                        \If{$\mathrm{dist}\left(e, e^\prime\right) < \delta$}
                            \State{$\mathrm{in\_range} = \truelit$}
                            \State\textbf{break}
                        \EndIf%
                        \If{$\neg \mathrm{in\_range}$}
                            \State{%
                                $\epsilon = \epsilon \cup \left\{ e \right\}$}
                        \EndIf%
                    \EndFor%
                \EndFor%
                \If{$\left|\epsilon\right| \geq \eta \left|\epsilon_0\right|$}
                    \State\textbf{break}
                \EndIf%
                \State{$\delta = \zeta\delta$}
            \EndFor%
            \State{\Return{$\epsilon$}}
        \EndFunction%
    \end{algorithmic}
    \caption{%
        The algorithm used to sample the Pareto frontier.
    }\label{lo:alg:sample}
\end{figure}

This method is superior to random sampling, because random sampling often
samples points that are close together, which usually are very similar
implementations.

We found that with all improvements above and a faster accuracy analysis in
Section~\ref{lo:sub:accuracy}, the algorithm is significantly faster than
the original optimization algorithm in Section~\ref{po:sub:discovering} of
Chapter~\ref{chp:progopt}.  Even though this algorithm may discover potentially
fewer candidates on the Pareto frontier, we can now explore greater partial
loop unrolling depths to widen the swing of the Pareto frontier in the same
amount of time.

% \todo{Move this algorithm to Chapter~stropt.}
% \begin{figure}[ht]
%     \centering
%     \begin{algorithmic}
% \Function{Optimize}{$op(e_1, e_2)$}
%     \State{$s_1 \gets$ \Call{Optimize}{$e_1$}}
%     \State{$s_2 \gets$ \Call{Optimize}{$e_2$}}
%     \State{$s^\prime \gets \varnothing$}
%     \State{%
%         $s \gets \left\{
%             op(e^\prime_1, e^\prime_2) \mid
%             e^\prime_1 \in s_1 \wedge e^\prime_2 \in s_2
%         \right\}$}
%     \While{$s \neq s^\prime$}
%         \State{$s^\prime \gets s$}
%         \State{$s^{\prime\prime} \gets \varnothing$}
%         \For{$r \in \mathrm{transformation\_rules}, e \in s$}
%             \For{%
%                 $e^\prime \text{~where~}
%                     e \stackrel{r}{\eqgenrel} e^\prime$
%             }
%                 \State{%
%                     $s^{\prime\prime} \gets
%                         s^{\prime\prime} \cup \left\{ e^\prime \right\}$}
%             \EndFor
%         \EndFor
%         \State{$s \gets$ \Call{Prune}{$s^{\prime\prime}$}}
%     \EndWhile
%     \State{\Return{$s$}}
% \EndFunction
%     \end{algorithmic}
%     \caption{%
%         The algorithm we used for the efficient discovery of equivalent
%         structures in \glspl{mir}.
%     }
% \end{figure}


\subsection{Transformation Rules}
\label{lo:sub:transformation_rules}

This section details the new transformation rules in the equivalence relation
$\equiv$ and consequently in the structural optimization relation $\eqgenrel$.
Each transformation rule on its own is not revolutionary, but for the
first time, they are used in tandem with arithmetic rules and control-flow
restructuring rules introduced respectively in Chapters~\ref{chp:stropt}
and~\ref{chp:progopt}.  This enables a much better automatic structural
optimization on the latency, resource usage and accuracy of numerical programs,
than is possible using only a subset of them.

In Chapters~\ref{chp:stropt} and~\ref{chp:progopt}, \soap{} provides a range
of equivalence rules that are used in the optimization, such as associativity,
distributivity, commutativity, constant propagation, and partial loop
unrolling.  In Table~\ref{lo:tab:rules}, we list those rules that proved
effective when minimizing loop latencies.  Although these rules are used to
transform \glspl{mir}, we present before-and-after examples written in C to
allow the effect of each rule to be readily understood.
\begin{table}[t]
    \caption{%
        Before-and-after examples to demonstrate the access reduction rules.
    }\label{lo:tab:rules}
    \centering
    \begin{tabular}{ll}
        \toprule
        \multicolumn{2}{c}{\textbf{Access Reduction Rules}}
        \\\midrule\midrule
        \Shade\emph{Multiple reads} & \Shade%
            \texttt{x=A[i-{}-]; y=A[i+1];} $~\eqgenrel~$
            \texttt{x=A[i-{}-]; y=x;}
        \\\midrule
        \emph{Multiple writes} &
            \texttt{A[i++]=x; A[i-1]=y;} $~\eqgenrel~$
            \texttt{A[i++]=y;}
        \\\midrule
        \Shade\emph{Read after write} & \Shade%
            \texttt{A[i++]=x; y=A[i-1];} $~\eqgenrel~$
            \texttt{A[i++]=x; y=x;}
        \\\midrule
        \emph{Indep.\ accesses} (where $\texttt{i}\not\equiv\texttt{j}$) &
            \texttt{A[i]=x; y=A[j];} $~\eqgenrel~$
            \texttt{y=A[j]; A[i]=x;}
        \\\bottomrule%
    \end{tabular}%
\end{table}%

Our new rules, the access reduction rules, with formal definitions below and
examples in Table~\ref{lo:tab:rules}, remove extraneous data-dependences that
arise after partial unrolling.  These rules, along with partial loop unrolling,
mostly do not really impact latency, because they are very well studied in
polyhedral loop dependence analysis, and tools such as \gls{vhls} can make use
of them automatically.  However, they give the necessary freedom to arithmetic
rules to affect latency.  The rules are as follows, where $A$ is an array,
$\bar{\imath}, \bar{\jmath}$ are subscripts, and $e, e^\prime$ are expressions:
\begin{itemize}

    \item \emph{Multiple reads}, eliminates the second of two reads of the
    same location.  This arises naturally from the \gls{mir}, as common
    subexpressions are shared.

    \item \emph{Multiple writes}, eliminates a write that is overwritten:
    \vspace{-11pt}
    \begin{equation}
        \update{\update{A}{\bar{\imath}}{e}}{\bar{\imath}}{e^\prime}
            \eqgenrel \update{A}{\bar{\imath}}{e^\prime}.
    \end{equation}

    \item \emph{Read after write}, eliminates a read from a location
    that has just been written:
    \vspace{-11pt}
    \begin{equation}
        \access{\update{A}{\bar{\imath}}{e}}{\bar{\imath}} \eqgenrel e.
    \end{equation}

    \item \emph{Independent accesses}, allows two array operations to be
    reordered if it can be proved that they never access the same location:
    \vspace{-11pt}
    \begin{equation}
        \access{\update{A}{\bar{\imath}}{e}}{\bar{\jmath}}
            \eqgenrel \access{A}{\bar{\jmath}},
        \text{if~} \bar{\imath} \not\equiv \bar{\jmath}.
    \end{equation}
    We also visualize this rule in Figure~\ref{lo:fig:indep_example}, which
    shows a sample \gls{mir} transformation.
\end{itemize}

\begin{figure}[ht]
    \begin{equation*}
        \begin{tikzpicture}[mir]
            \node[mirnode] (var_y) at (0,0) {\texttt{y}};
            \node[mirnode] (access)[right=of var_y] {$\accessop$};
            \node[mirnode] (j1)    [below right=of access] {\texttt{j}};
            \node[mirnode] (update)[below left=of access] {$\updateop$};
            \node[mirnode] (var_A) [left=of update] {\texttt{A}};
            \node[mirnode] (a2)    [below left=of update] {\texttt{A}};
            \node[mirnodealt] (i2) [below=5mm of update] {\texttt{i}};
            \node[mirnode] (x2)    [below right=of update] {\texttt{x}};

            \draw[|->] (var_y) -- (access);
            \draw[|->] (var_A) -- (update);
            \draw[<-] (access) -- (j1);
            \draw[<-] (access) -- (update);
            \draw[<-] (update) -- (x2);
            \draw[<-] (update) -- (i2);
            \draw[<-] (update) -- (a2);
            \brackets{(current bounding box)}
        \end{tikzpicture}
        \eqgenrel
        \begin{tikzpicture}[mir]
            \node[mirnode] (var_y) at (0,0) {\texttt{y}};
            \node[mirnode] (access)[right=of var_y] {$\accessop$};
            \node[mirnode] (j1)    [below right=of access] {\texttt{j}};
            \node[mirnode] (update)[below left=of access] {$\updateop$};
            \node[mirnode] (var_A) [left=of update] {\texttt{A}};
            \node[mirnode] (a2)    [below left=of update] {\texttt{A}};
            \node[mirnodealt] (i2) [below=5mm of update] {\texttt{i}};
            \node[mirnode] (x2)    [below right=of update] {\texttt{x}};

            \draw[|->] (var_y) -- (access);
            \draw[|->] (var_A) -- (update);
            \draw[<-] (access) -- (j1);
            \draw[<-] (update) -- (x2);
            \draw[<-] (update) -- (i2);
            \draw[<-] (update) -- (a2);
            \draw[<-, channel] (access) to[bend left=30] (a2);
            \brackets{(current bounding box)}
        \end{tikzpicture}
    \end{equation*}
    \caption{%
        A sample \acrshort{mir} transformation using the \emph{independent
        accesses} rule.
    }\label{lo:fig:indep_example}
\end{figure}

These rules may not seem powerful on their own, but when combined with other
structural rules, they enable \soap{} to detect dependences that can be removed
in the \gls{mir}\@.  This in turn allows more opportunities for the rules
to further reduce loop latency.  By way of illustration, we optimize the
Fibonacci series example program in Figure~\ref{lo:fig:fib} for latency.
By partially unrolling the loop with a factor 2, we obtain the program in
Figure~\ref{lo:fig:fib2}.  We can see that because of the rigid array access
pattern, associativity cannot be applied easily to the loop kernel.  However,
by applying the above access reduction rules first, we give associativity the
freedom to reduce latency by half and improve accuracy by 50\%, as shown in
Figure~\ref{lo:lst:fib_opt}.
\begin{figure}[ht]
    \centering
    \begin{lstlisting}
    for (int i = 2; i < 1023; i += 2) {
        float t2 = A[i - 2], t3 = A[i - 3];
        A[i - 1] = t2 + t3;
        A[i] = 2 * t2 + t3;
    }
    \end{lstlisting}
    \caption{%
        The optimized program that computes the Fibonacci sequence.  It reduces
        latency of the original in Figure~\ref{lo:fig:fib} by half and improves
        accuracy by 50\%.
    }\label{lo:lst:fib_opt}
\end{figure}

Without the above access reduction rules, it is therefore not possible to
reach this optimized implementation.  Conversely, it is not possible to
relax scheduling constraints due to inter-iteration dependences without
arithmetic equivalence rules, as these reduction rules are there to assist
transformation rules that make a difference in latency.  Therefore the rules
in Table~\ref{lo:tab:rules} must be used in conjunction with arithmetic and
control-flow rules to optimize latency in numerical programs.
