\section{Structural Optimization}
\label{lo:sec:structural_optimization}

From a numerical program, we can generate an \gls{mir} using the translation
in Section~\ref{lo:sec:intermediate}.  The next step is to transform the
\gls{mir}, and discover \glspl{mir} that are equivalent to the original
\gls{mir} in real arithmetic, but may execute differently in finite-precision
arithmetic because of round-off errors.

\subsection{Algorithm}
\label{lo:sub:algorithm}

% Our optimization starts by partitioning the \gls{mir} into sub-\glspl{mir}.
% This further reduces the size of the search space, \ie~equivalent \glspl{mir}
% reachable using our transformation rules.  This speeds up the rest of the
% optimization process, because these rules are not applied on partition
% boundaries.

As discussed in Section~\ref{lo:sec:introduction}, even a small expression
could have a huge number of equivalent ones.  Exhaustively discovering
all equivalent \glspl{mir} would result in combinatorial explosion of the
number of equivalent \glspl{mir} in the search space.  For this reason, we
base ourselves on an algorithm from \soap{} that search efficiently, by
discovering equivalences in a bottom-up hierarchy.  In this section we discuss
the improvements we have made to the algorithm which further increases the
performance of this algorithm.

Our first contribution is that instead of optimizing the \gls{mir} immediately,
we start by partitioning the \gls{mir} into multiple smaller sub-\glspl{mir}.
In turn, each are optimized separately and generate a set of equivalent
sub-\glspl{mir}.  We then select combinations from these sub-\glspl{mir}
to be merged, this generates a set of \glspl{mir} that are equivalent to
the original.  Finally, we preserve those \glspl{mir} merged on the Pareto
frontier.

Figure~\ref{lo:alg:optimize} shows the pseudocode of the optimization
algorithm, it takes as an input an \gls{mir} graph, and produces a set of
equivalent graphs that are estimated to be Pareto-optimal when converted into
C programs and synthesized into circuits.  Although this algorithm deals with
a special case, \ie~a root node $op$ with two child subtrees $e_1, e_2$, it
can easily be generalized to an arbitrary number of child subtrees.  Here,
$e \stackrel{r}{\rightsquigarrow} e'$ means $e^\prime$ can be obtained by
transforming part of the graph $e$ in accordance with the transformation rule
$r$.  The next section discusses the transformation rules we used.

The algorithm starts by discovering equivalences in the leaves of an \gls{mir},
and progress upwards for equivalent structures of the individual components
that make up the graph, until the roots of the graph, where we have a set of
equivalent \glspl{mir} to the original \gls{mir}\@.  As it traverses through
the \gls{mir}, the algorithm calculates the performance metrics at each node,
using the analyses presented in the next section.  Transformations that are not
Pareto-optimal are immediately pruned from the search space, thus reducing the
average complexity of the algorithm.

Our second contribution is the \textsc{Prune} function.  We rely on this
function to efficiently steer the direction of our Pareto frontier as
we discover new candidates.  It takes as an input the set of equivalent
\glspl{mir} that we have discovered, and prune \glspl{mir} in this set to
reduce its size, this keeps the size of discovered \glspl{mir} tractable.  The
\soap{} framework prunes the \glspl{mir} that are Pareto-suboptimal, leaving
only those that are on the Pareto frontier.  However, because our Pareto
frontier is three-dimensional, there is a large increase in the number of
Pareto-optimal \glspl{mir}.  This Pareto pruning approach is no longer feasible
for our benchmark examples.  To tackle this, we introduce another step in
\textsc{Prune} to further decrease the number of \glspl{mir} in the set by
sampling.  We developed a new sampling algorithm, inspired by Poisson-disk
sampling algorithm~\cite{bridson07}, which samples the Pareto frontier by first
randomly selecting one point, then iteratively grows the set of points by
adding the neighbours from the point that are separated by at lease a certain
distance, we search by bisection for the distance that keeps 20\% of all point
in the Pareto frontier.  This method is superior to random sampling, because
random sampling often samples points that are close together, which usually are
very similar implementations.

We found that with our improvements, the algorithm is significantly faster than
the original optimization algorithm in \soap, with a 5-fold increase in speed,
at a cost of less points on the Pareto frontier.

\todo{Move this algorithm to Chapter~stropt.}
\begin{figure}[ht]
    \centering
    \begin{algorithmic}
\Function{Optimize}{$op(e_1, e_2)$}
    \State{$s_1 \gets$ \Call{Optimize}{$e_1$}}
    \State{$s_2 \gets$ \Call{Optimize}{$e_2$}}
    \State{$s^\prime \gets \varnothing$}
    \State{%
        $s \gets \left\{
            op(e^\prime_1, e^\prime_2) \mid
            e^\prime_1 \in s_1 \wedge e^\prime_2 \in s_2
        \right\}$}
    \While{$s \neq s^\prime$}
        \State{$s^\prime \gets s$}
        \State{$s^{\prime\prime} \gets \varnothing$}
        \For{$r \in \mathrm{transformation\_rules}, e \in s$}
            \For{%
                $e^\prime \text{~where~}
                    e \stackrel{r}{\rightsquigarrow} e^\prime$
            }
                \State{%
                    $s^{\prime\prime} \gets
                        s^{\prime\prime} \cup \left\{ e^\prime \right\}$}
            \EndFor
        \EndFor
        \State{$s \gets$ \Call{Prune}{$s^{\prime\prime}$}}
    \EndWhile
    \State{\Return{$s$}}
\EndFunction
    \end{algorithmic}
    \caption{%
        The algorithm we used for the efficient discovery of equivalent
        structures in \glspl{mir}.
    }
    \label{lo:alg:optimize}
\end{figure}

\subsection{Transformation Rules}
\label{lo:sub:transformation_rules}

This section details the transformation rules used in our structural
optimization algorithm in Figure~\ref{lo:alg:optimize}.  These transformation
rules, each on its own is not revolutionary, but for the first time, we bring
them together to show a much better automatic structural optimization on the
latency, resource usage and accuracy of numerical programs, than using only a
subset of them.

\begin{table}[t]
\newcommand\tstack[1]{\begin{tabular}[t]{@{}l@{}}#1\end{tabular}}
    \centering
\begin{tabular}{@{}l@{~~~}l@{}}
\hline
\multicolumn{2}{c}{\bf Arithmetic Rules}
\\\hline
\emph{Associativity} & \texttt{(a + b) + c} $~\rightsquigarrow~$ {\tt a + (b +
c)}
\\\hline
\emph{Commutativity} & \texttt{a + b} $~\rightsquigarrow~$ \texttt{b + a}
\\\hline
\emph{Distributivity} & \texttt{(a + b) * c} $~\rightsquigarrow~$ \texttt{a * c + b * c}
\\\hline
\emph{Negation} & \texttt{a - b} $~\rightsquigarrow~$ \texttt{a + (-b)}
\\\hline
\emph{Subtraction} & \texttt{(a + b) - (a + b)} $~\rightsquigarrow~$ \texttt{0}
\\\hline
\emph{Const.\ prop.} & \texttt{(a * b + c / d) * 0} $~\rightsquigarrow~$ \texttt{0}
\\\hline
\emph{Division} & \texttt{a / (5 / b)} $~\rightsquigarrow~$ \texttt{a * b * 0.2}
\\\hline\hline
\multicolumn{2}{c}{\bf Control Flow Restructuring Rules}
\\\hline
\emph{Partial loop unrolling} & \tstack{\texttt{for(i=0;i<1000;i++)\string{C\textsubscript{i};\string}} $~\rightsquigarrow~$ \\ \texttt{for(i=0;i<1000;i+=2)\string{C\textsubscript{i}; C\textsubscript{i+1};\string}}}
\\\hline\hline
\multicolumn{2}{c}{\bf Access Reduction Rules}
\\\hline
\emph{Multiple reads} & \texttt{x=A[i-{}-]; y=A[i+1];} $~\rightsquigarrow~$ \texttt{x=A[i-{}-]; y=x;}
\\\hline
\emph{Multiple writes} & \texttt{A[i++]=x; A[i-1]=y;} $~\rightsquigarrow~$ \texttt{A[i++]=y;}
\\\hline
\emph{Read after write} & \texttt{A[i++]=x; y=A[i-1];} $~\rightsquigarrow~$ \texttt{A[i++]=x; y=x;}
\\\hline
\emph{Indep.\ accesses} (where $\texttt{i}\not\equiv\texttt{j}$) & \texttt{A[i]=x; y=A[j];} $~\rightsquigarrow~$ \texttt{y=A[j]; A[i]=x;}
\\\hline
\end{tabular}
\caption{%
    Before-and-after examples to demonstrate the transformation rules
    we used. The arithmetic and control flow rules are inherited from
    Chapter~\ref{chp:progopt}; the access reduction rules are introduced in
    this work.}\label{lo:tab:rules}
\end{table}

\soap{} provides a range of equivalence rules that are used in the
optimization, such as associativity, distributivity, commutativity, constant
propagation, and partial loop unrolling.  In Table~\ref{lo:tab:rules}, we list
those rules that proved effective when minimizing loop latencies.  Although
these rules are used to transform \glspl{mir}, we present before-and-after
examples written in C to allow the effect of each rule to be readily
understand.

Our new rules, the access reduction rules, with formal definitions below and
examples in Table~\ref{lo:tab:rules}, remove extraneous data dependences that
arise after partial unrolling.  These rules, along with partial loop unrolling,
mostly do not really impact latency, because they are very well studied in
polyhedral compilation, and tools such as \gls{vhls} can make use of them
automatically.  However, they give the necessary freedom to arithmetic rules
to affect latency.  The rules are as follows, where $A$ is an array, $\bar{i},
\bar{j}$ are subscripts, and $e, e^\prime$ are expressions:
\begin{itemize}

    \item \emph{Multiple reads}, eliminates the second of two reads of the same
    location.  It arises naturally from the \gls{mir}, as common subexpressions
    are shared.

    \item \emph{Multiple writes}, eliminates a write that is overwritten:
    \begin{equation}
        \update{\update{A}{\bar{i}}{e}}{\bar{i}}{e^\prime}
            \rightsquigarrow \update{A}{\bar{i}}{e^\prime}
    \end{equation}

    \item \emph{Read after write}, eliminates a read from a location
    that has just been written:
    \begin{equation}
        \access{\update{A}{\bar{i}}{e}}{\bar{i}} \rightsquigarrow e
    \end{equation}

    \item \emph{Independent accesses}, allows two array operations to be
    reordered if it can be proved that they never access the same location:
    \begin{equation}
        \access{\update{A}{\bar{i}}{e}}{\bar{j}}
            \rightsquigarrow \access{A}{\bar{j}},
        \text{if~} \bar{i} \not\equiv \bar{j}
    \end{equation}
    We visualize this rule also in the following sample \gls{mir}
    transformation:
    \begin{equation}
        \label{lo:eq:indep_reads_mir}
        \begin{tikzpicture}[mir]
            \node[mirnode] (var_y) at (0,0) {\texttt{y}};
            \node[mirnode] (access)[right=of var_y] {$\accessop$};
            \node[mirnode] (j1)    [below right=of access, xshift=-2mm] {\texttt{j}};
            \node[mirnode] (update)[below left=of access, xshift=6mm] {$\updateop$};
            \node[mirnode] (var_A) [left=of update] {\texttt{A}};
            \node[mirnode] (a2)    [below left=of update] {\texttt{A}};
            \node[mirnode] (i2)    [below=of update] {\texttt{i}};
            \node[mirnode] (x2)    [below right=of update] {\texttt{x}};

            \draw[|->] (var_y) -- (access);
            \draw[|->] (var_A) -- (update);
            \draw[<-] (access) -- (j1);
            \draw[<-] (access) -- (update);
            \draw[<-] (update) -- (x2);
            \draw[<-] (update) -- (i2);
            \draw[<-] (update) -- (a2);
            \brackets{(current bounding box)}
        \end{tikzpicture}
        \rightsquigarrow
        \begin{tikzpicture}[mir]
            \node[mirnode] (var_y) at (0,0) {\texttt{y}};
            \node[mirnode] (access)[right=of var_y] {$\accessop$};
            \node[mirnode] (j1)    [below right=of access, xshift=-2mm] {\texttt{j}};
            \node[mirnode] (update)[below left=of access] {$\updateop$};
            \node[mirnode] (var_A) [left=of update] {\texttt{A}};
            \node[mirnode] (a2)    [below left=of update] {\texttt{A}};
            \node[mirnode] (i2)    [below=of update] {\texttt{i}};
            \node[mirnode] (x2)    [below right=of update] {\texttt{x}};

            \draw[|->] (var_y) -- (access);
            \draw[|->] (var_A) -- (update);
            \draw[<-] (access) -- (j1);
            \draw[<-] (access) to[bend left=20] (a2);
            \draw[<-] (update) -- (x2);
            \draw[<-] (update) -- (i2);
            \draw[<-] (update) -- (a2);
            \brackets{(current bounding box)}
        \end{tikzpicture}
    \end{equation}

\end{itemize}

These rules may not seem powerful on their own, but when combined with other
structural rules, they enable our tool to detect dependences that can be
removed in the \gls{mir}\@.  This could in turn allow more opportunities
for the rules to further reduce loop latency.  By way of illustration, we
optimize the following program for latency, which computes a Fibonacci series
generalized to real numbers:
\begin{lstlisting}
    for (int i = 2; i < 1023; i++)
        A[i] = A[i - 1] + A[i - 2];
\end{lstlisting}
If we simply partially unroll this loop, we can see that because of the
rigid array access pattern, associativity cannot be applied easily to the
loop kernel.
\begin{lstlisting}
    for (int i = 2; i < 1023; i++) {
        A[i] = A[i - 1] + A[i - 2];
        A[i + 1] = A[i] + A[i - 1];
    }
\end{lstlisting}
By applying the above access reduction rules first, we give associativity the
freedom to reduces latency by half and improves accuracy by 50\%:
\begin{lstlisting}
    for (int i = 2; i < 1023; i += 2) {
        float t1 = A[i - 1], t2 = A[i - 2];
        A[i] = t1 + t2;
        A[i + 1] = 2 * t1 + t2;
    }
\end{lstlisting}
Therefore, without the above access reduction rules, it is not possible to
reach this optimized implementation.  Conversely, it is not possible to relax
scheduling constraints due to inter-iteration dependences without arithmetic
equivalence rules, as these reduction rules are there to assist transformation
rules that could really make a difference in latency.  Therefore all these
rules in Table~\ref{lo:tab:rules} are essential to the optimization of latency
in numerical programs.
