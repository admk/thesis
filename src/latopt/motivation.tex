\section{Motivation}
\label{lo:sec:motivation}

\newcommand\barr[3]{\texttt{\underline{#1[#2][#3]}}}

Figure~\ref{lo:fig:seidel_prog} gives an implementation of the Seidel stencil
computation, extracted from PolyBench~\cite{polybench}, where initially all
values in the array \verb|A| are single-precision floating-point values between
$0$ and $1$.  It resembles the typical code frequently used in fluid dynamic
simulations for solving partial differential equations and systems of linear
equations.

\begin{figure}[ht]
    \begin{lstlisting}
        #define N 1024
        for (int t = 0; t < 20; t++)
          for (int i = 1; i < N-1; i++)
            for (int j = 1; j < N-1; j++)
              $\barr{A}{i}{j}$ = 0.2 * (A[i-1][j] + $\barr{A}{i}{j-1}$ +
                  A[i][j] + A[i][j+1] + A[i+1][j]);
    \end{lstlisting}
    \caption{%
        An excerpt from the Seidel stencil~\cite{polybench}.  The
        inter-iteration data-dependence of the innermost loop is underlined
        ($\barr{A}{i}{j}$ and $\barr{A}{i}{j-1}$).
    }\label{lo:fig:seidel_prog}
\end{figure}

We start by synthesizing this program in \gls{vhls}\@.  We enable \emph{loop
pipelining} in \gls{vhls}, which asks it to optimize the loop by overlapping
its iterations.  However, we can observe that this program has very limited
opportunity for pipelining, because each iteration \verb|j| of the innermost
loop ends by writing to \verb|A[i][j]|, and the next iteration \verb|j+1|
begins by reading from \verb|A[i][j]|; this inter-iteration dependence is
highlighted in Figure~\ref{lo:fig:seidel_prog}.  Hence, it serves as our
example to motivate a better \soap~to efficiently pipeline loops.

\Gls{vhls} generates a schedule where the depth of the loop $D$ is $49$, and
\gls{ii} as enforced by the data-dependences above is $46$.  The trip count of
the innermost loop is $N = 1022$.  The overall latency of the innermost loop is
therefore $((N-1) \times \II) + D = 47,015$ cycles.

We then enable \gls{vhls}'s \gls{eb} optimization.  When synthesized, this
optimization pass tries to reorder the sequence of additions in the loop body
into a tree structure, thus reducing the $\II$ to 28 cycles, and the depth $D$
to 42 cycles, while the trip count $N = 1022$ remains the same.  The overall
latency is now $((N-1) \times \II) + D = 28,630$ cycles.  The overall resource
usage remains roughly the same.

However, as mentioned in Section~\ref{bg:sec:discovering_equivalent_programs}
of Chapter~\ref{chp:background}, \gls{vhls}'s \gls{eb} has two shortcomings.
Firstly, it is not aware of the inter-iteration data-dependence and misses the
opportunity to further pipeline this loop.  Secondly, and most importantly,
\gls{vhls} does not guarantee that this optimization will not result in
catastrophic numerical inaccuracies.

We further discover that if the loop is partially unrolled, \gls{vhls}'s
\gls{eb} did not improve the total run time, despite using a lot more
resources.  Additionally, \gls{eb} only makes use of associativity, but
not other equivalence rules.  These limitations pose great restrictions on
\gls{vhls}'s ability to produce a significantly faster implementation.

We then use the enhanced \soap~of this chapter to automatically discover
equivalent programs from the program in Figure~\ref{lo:fig:seidel_prog}.
Because \soap~explores a large number of paths that lead to a Pareto frontier
of implementations, here we illustrate one of the many paths that could be
taken by minimizing latency, while trying to optimize accuracy and resource
usage.  By using just arithmetic equivalences, \soap~specifically applies
transformations to alleviate the constraints on the inter-iteration dependence,
and discovers that the innermost loop can be rewritten to minimize latency as
shown in Figure~\ref{lo:fig:seidel_prog_2}.

\begin{figure}[ht]
    \begin{lstlisting}
        for (int j = 1; j < 1023; j++)
            A[i][j] = 0.2 * (A[i][j-1] +
                ((A[i][j] + A[i][j+1]) +
                 (A[i+1][j] + A[i-1][j])));
    \end{lstlisting}
    \caption{The optimized program using only arithmetic equivalences.}
    \label{lo:fig:seidel_prog_2}
\end{figure}

Although this loop still has a data-dependence between consecutive iterations,
this transformation greatly reduces latency because most of the loop iterations
can now be overlapped.  We find that this simple transformation can reduce
$\II$ to 19, which speeds up the original program by 2.3$\times$, using almost
the same number of \glspl{lut} and \gls{dsp} elements as the original program.
At the same time, the sequence of additions are now reordered to minimize
round-off errors, improving the accuracy by 18\%.

\soap~also supports more complex control-flow restructuring transformations,
such as partial loop unrolling, in tandem with rules that optimize memory
accesses and arithmetic calculations.  This can further reduce the loop's
latency.  In this example, unrolling the loop by a factor of two (\ie~updating
two matrix elements on every iteration and halving the trip count) and applying
other rules, results in a program with $\II = 19, D = 152, N = 511$.  When
implemented on a device it is 4.8$\times$ faster than the original, and
almost twice as accurate, at a cost of 17\% more \glspl{lut}, as shown in
Figure~\ref{lo:fig:seidel_prog_3}.

\begin{figure}[ht]
\begin{lstlisting}
  for (int j = 1; j < 1023; j += 2) {
    float t0 = A[i][j-1], t1 = A[i][j+1];
    float t2 = (A[i][j] + t1) + (A[i+1][j] + A[i-1][j]);
    float t3 = 0.04f * t2 + 0.2f *
        ((t1 + A[i][j+2]) + (A[i+1][j+1] + A[i-1][j+1]));
    A[i][j] = 0.2f * (t0 + t2);
    A[i][j+1] = 0.04f * t0 + t3;
  }
\end{lstlisting}
    \caption{The optimized program using arithmetic equivalences in tandem with
    control-flow restructuring and memory access optimization.}
    \label{lo:fig:seidel_prog_3}
\end{figure}

Further increasing the optimization effort, which enables the loop to be
deeper partially unrolled, leads to a program that is 7$\times$ faster
than the original, but uses 2.8$\times$ \glspl{lut}.  To summarize, in
Table~\ref{lo:tab:seidel_results}, we compare \gls{vhls} with \gls{eb}, against
one of the many implementations that we have explored using \soap~with the
increased optimization effort.  The three columns respectively shows the
original program with loop pipelining enabled, what \gls{vhls} can achieve
alone, and the capability of \soap.  It is important to note that the round-off
error is unknown for \gls{vhls} with \gls{eb}, because it cannot predict the
impact of its unsafe optimizations on accuracy.  We performed place-and-route
for exact statistics.

\begin{table}[ht]
    \centering\singlespacing%
    \renewcommand\arraycolsep{4.0mm}
    \caption{%
        Comparison among the optimized implementations generated by
        \gls{vhls}'s expression balancing and our optimizer.  The row ``Total
        run time (s)'' indicates the wall-clock time in seconds of running the
        synthesized circuits.
    }\label{lo:tab:seidel_results}
    $\begin{array}{rrrr}
        \toprule
        \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{\text{\gls{vhls}}} &
        \multicolumn{1}{c}{\text{\gls{vhls} with \gls{eb}}} &
        \multicolumn{1}{c}{\text{\gls{vhls} with \soap}}
        \\ \midrule\midrule

        \text{Clock period (ns)} &
        2.60 & 2.65 & 2.66 \\ \midrule

        \Shade \text{Inner latency (cycles)} &
        \Shade 47.0\unitk & \Shade 28.6\unitk & \Shade 6.59\unitk \\ \midrule

        \text{Total run time (s)} &
        2.50\unitm & 1.56\unitm & 0.358\unitm \\ \midrule

        \Shade \text{\glspl{lut}} &
        \Shade 620 & \Shade 623 & \Shade 1778 \\ \midrule

        \text{\gls{dsp} elements} &
        5 & 5 & 8 \\ \midrule

        \Shade \text{Round-off error} &
        \Shade 10.68\unitmu &
        \Shade \text{unknown} &
        \Shade 4.31\unitmu \\ \bottomrule
    \end{array}$
\end{table}
