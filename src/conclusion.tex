High-level synthesis tools are typically designed to adhere to a rigid
specification which outlines their behaviour.  It is a traditional practice
to design this specification and the subsequent tool to ensure that the
synthesized circuits perform functionally identical to the original source
program written in high-level language.  It is also viewed as a good practice
because it has predicable outcomes.  Guided by the rules of the language,
programmers translate mathematical objects such as algorithms and physical
information respectively into source code and numerical data, in a way similar
to tools adhering to their specifications.  This manual process of translation
is unfortunately an approximate one.  Computations as simple as $\sqrt{3}$ must
be approximated, \eg~they are carried out in floating-point arithmetic, because
of the finite nature of computing machines.  Therefore, \gls{hls} tools cannot
be relied upon for an exact interpretation of the mathematical objects we wish
to implement, even if they guarantee the functional equivalence between the
source code and the synthesized result.

Despite the awareness of the approximate characteristic of numerical
software/hardware implementations using floating-point operations, engineers
often take the risks of neglecting this fact, and anticipate their designs to
behave identically to the mathematical algorithms visioned in real arithmetic
within a reasonable but not well-defined error margin.  As it was shown in
Section~\ref{bg:sub:expression_accuracy} in Chapter~\ref{chp:background},
round-off errors when accumulated, could have detrimental effects on our daily
life.  The aforementioned functional equivalence between source and circuit
guaranteed by \gls{hls} tools is therefore unable to regain any lost accuracies
due to approximation.

Traditional \gls{ir}-level \gls{hls} program optimization consist of a series
of transformation passes.  Most of these passes do not predict whether
they have negative impact on the resulting circuit, and they limit their
capabilities by preserving functional equivalence.  Varying the order of
these passes could have significant impact on the quality, as these passes
interact with one another in a complicated manner, it is difficult to predict
the overall impact on performance~\cite{huang15}.  For $n$ passes, there are
$n{\,!}$ distinct ways to order, it is thus a considerable challenge to decide
the optimal pass ordering, which is exacerbated by the fact that it could be
highly dependent on the input program~\cite{cong13}.

These above shortcomings of traditional \gls{hls} tools and optimizing
compilers provide a strong motivation for the work proposed in this thesis.

Firstly, we can apply the philosophy of relaxing the functional equivalence
required by \gls{hls} tools.  In the mean time, we preserve the equivalence of
the underlying mathematical objects in real arithmetic which hardware designs
are approximating.  One can often improve the numerical accuracy by choosing a
better alternative among these equivalences.

Secondly, by the same paradigm shift, a wide range of optimization
opportunities can be explored to minimize throughput and resource utilization.
These opportunities were previously lost out to the necessity of ensuring
consistent behaviour.

Finally, optimization can be carried out by applying steps of equivalence
rewrites driven by a prediction model.  Traditional optimization passes can
be broken up into much smaller common parts made of equivalence rules can be
easily proved mathematically correct.  By using models to predict run time,
resources and accuracy to guide the optimization process, it is possible to
explore multiple designs that trade-off the three performance metrics while
removing concerns about the ordering problem.  Many optimization passes, such
as constant propagation, dead code removal, common subexpression elimination
and~\etc, are naturally subsumed by the new approach.  As the computational
power of machines increases exponentially, we can foresee an increase in the
scale of the vast search space to be explored in the future.

This thesis therefore broadens the horizon of \gls{hls} tools, and equips
them with the new program optimization paradigm by leveraging these above
observations.  Specifically, the trade-off relationship among numerical
accuracy, resource utilization and throughput are optimized in floating-point
numerical programs for \gls{hls}\@.  Here we summarize the contributions of
this thesis.

To the best of our knowledge, this thesis is the first to introduce
multiple-objective performance optimization in a unified framework for
discovering equivalence in programs.  Chapter~\ref{chp:stropt} implements
this framework and optimizes a suite of expressions that are difficult to
optimize by hand, and improve numerical accuracy and area automatically.
In the experimental results, it turns out that the two central goals,
\ie~improving accuracy and minimizing area, are often not in conflict, as
optimized expressions can enjoy almost all enhancements that can be achieved
in both metrics.  Guided by the concept of abstract interpretation, it further
introduces the semantics-based program analyses to jointly reason about
safe ranges of round-off errors and resource utilization, and subsequently,
discovery of equivalent expressions.  This technique lays the necessary
foundation for program equivalence beyond simple arithmetic expressions.

The infinite size of the equivalent program space, coupled with undecidability
of program properties, makes the program optimization an even more
challenging task than the one of arithmetic expressions.  For this,
Chapter~\ref{chp:progopt} introduces a new graph-based intermediate
representation, \gls{mir}, for capturing the semantics of numerical programs.
This approach reduces the size of the search space, and the \gls{ir} itself
is derived from the formal semantics of programs to ensure the correctness
of equivalent \glspl{mir} and the back-and-forth translation between C and
\gls{mir}\@.  It further eliminates the problem of optimization pass ordering,
because by using the equivalence discovery framework, the Pareto frontier can
be extended incrementally with small steps of rewrites to multiple candidates.
Traditional compiler optimizations are naturally subsumed and further enhanced
by the \glspl{mir}, as many optimization techniques such as loop splitting and
loop fusion that previously must be profiled to justify enabling them, can
emerge automatically from the optimization process.  By optimizing a suite of
resource-efficient benchmark examples, the tool improves the numerical accuracy
by up to 65\%.

Formerly, \gls{hls} tools' ability to pipeline loops is fundamentally constrained
by intra-iteration dependencies.  Traditional optimization techniques such as
partial loop unrolling may have minimal effects on the initiation interval of
pipelined loops, as these do not impact the data-path structure, which ensures
that the functional equivalence is preserved.  Encouraged by the promising
effects of Nicolau~\etal's tree height reduction technique~\cite{nicolau91}
and LegUp's recurrence minimization~\cite{canis14}, Chapter~\ref{chp:latopt}
further incorporates latency analysis into the unified program optimization
framework.  It was found that traditional optimization techniques when used
in tandem with the arithmetic equivalence rules and memory access reduction
rules can significantly improve the latency and accuracy of a numerical
program.  In Chapter~\ref{chp:progopt}, the experimental results identifies
that the static analysis of round-off errors for each candidate explored is
the key factor to the speed of optimization.  This problem is addressed in
this chapter by graph partitioning and candidate pruning algorithms.  It
further enables deeper partial loop unrolling factors that was not explored in
Chapter~\ref{chp:progopt}.  Often as we optimize numerical programs by spending
more resources, latency and round-off error can be simultaneously minimized, as
more resources would allow greater flexibility to discover equivalent programs
that often perform well in terms of run time and accuracy.  By optimizing
a suite of benchmark examples from PolyBench and Livermore loops, the tool
improves the latency and accuracy of each by up to 12$\times$ and 7$\times$
respectively, at a cost of 4$\times$ more resource utilization.


\section{Future Prospects}
\label{cc:sec:future_prospects}

In its current form, the new approach to program optimization explained in this
thesis forms the underlying basis for a much larger set of future work.  Even
though it is precursory on its own, the promising experimental results showcase
the powerful optimization it can bring to optimizing compilers and \gls{hls}
tools.  Here, a list of potential directions of future research is discussed
that could further widen the scope of our technique for a broader range of
applications.

\textbf{\gls{llvmir}-Level Program Optimization.}  We could envision
a back-and-forth translator from \gls{llvmir}~\cite{llvm, llvm_ir} to
\gls{mir} graphs.  This could enable a much wider applicability of the
technique presented in the thesis to both \gls{llvm}-based \gls{hls} tools
and software compilers.  Additionally, it could benefit from existing
\gls{llvm} optimizations passes by using the optimized \gls{llvmir} code
as inputs.  There are however obstacles in migrating to \gls{llvmir} as
the source language.  Firstly, \gls{llvmir} is \gls{ssa}-based.  Since it
uses temporary variables for intermediate results in computation, a full
liveness analysis~\cite{hathhorn12, nielson99, boissinot08} may be necessary
to eliminate temporary variables from the resulting \gls{mir}\@.  Secondly,
control-flows in \gls{llvmir} are more freely structured.  Unlike C, which
defines \iflit~statements and \whilelit~loops and discourages the use of
\verb|goto| statements, control-flow in \gls{llvmir} are composed by basic
blocks and branches between pairs of them.  This requires the \gls{mir} to be
further extended to cope with complex control-flow patterns.  Conventionally,
programs written with branches are often analyzed using \emph{continuation
style semantics}~\cite{felleisen88}.  It is not evident how this semantics can
be embedded within \glspl{mir}.

\textbf{Tighter bounds on round-off errors.} As an alternative for interval
analysis, the accuracy analysis could enjoy more sophisticated abstract domains
that capture the correlations between variables, and produce tighter bounds
for results.  Currently, the analysis cannot produce meaningful, \ie~finite,
bounds on the round-off errors of certain numerical programs.  If the analysis
fails to bound errors, then currently the optimization cannot be directed
to a more accurate implementation.  By using abstract relational domains,
it is possible to produce a much tighter bound on the values of program
variables, and the associated errors.  There are a few relational domains-based
static analysis techniques of floating-point errors~\cite{mine07_2, putot04,
goubault11, astree}, however making use of them still poses challenges.  Each
floating-point operation introduces an independent error term as a new variable
in the formulation of these relational domains, and it may be difficult to
determine how to collapse these error terms into a smaller set of variables,
as the optimization in this thesis can introduce a large number of error
variables.

\textbf{Special and fused operators.} There could be a lot of interest in the
\gls{hls} community on how \soap~can be incorporated with existing work on
fused floating-point data-path synthesis.  Langhammer~\etal~\cite{langhammer}
propose that normalization and denormalization stages could be regarded as
redundant between operators in a floating-point data-path.  By removing
these stages, subsets of the data-path become fixed-point data-paths, in the
meanwhile saving resources and improving throughput at a cost of accuracy.  It
will be compelling to isolate the normalization/denormalization stages into
operators in the \soap~framework, so that a mixed floating-point/fixed-point
program can more efficiently trade-off resources, accuracy and latency.

\textbf{Multiple word-lengths.}  In this thesis, experiments have been carried
out on floating-point operations with a fixed mantissa only.  It would be
beneficial to further integrate fixed-point support.  Additionally, by further
supporting multiple precisions in the data-path, \ie~allowing each operators to
compute with different precisions, the trade-off relationship among our three
primary performance measures can be even more effective.  Techniques, known
as multiple word-length optimization~\cite{constantinides, lee06, cantin02},
exist to apply a heuristic approach to perturb the precisions in a data-path,
so that a performance metric can be optimized while round-off errors of outputs
satisfies an error budget.  Instituting such techniques in the \soap~framework
is rewarding as it can further reduce the area and latency requirement of a
synthesized circuit for a given accuracy.  All of these approaches optimize a
fixed data-path, whereas in \soap~the structure of the data- and control-paths
are varying as we optimize them.  Analyzing each of the candidates for the
optimal precision assignment to each operator is very inefficient because of
the number of candidates explored.  Moreover, current techniques work with a
predetermined error budget, and yet in fact a Pareto frontier exists for each
data-path to trade-off accuracy, resources and latency.

\textbf{Numerical analysis and linear algebra.}  There are two distinct
approaches to the analysis of round-off errors.  One focuses on the round-off
errors by statically analyzing numerical programs, and apply this in a way
which is as general as possible, similar to the method presented in this
thesis.  On the other hand, there are techniques employed by numerical analysts
to evaluates and improve the numerical accuracy and stability of particular
algorithms analytically.  Many creative solutions to challenges are invented
in this process.  For instance, \emph{Kahan's compensated summation} algorithm
is an accurate way to compute a sum of $n$ values, $\sum_{i = 0}^{n-1}
x_i$~\cite{kahan65} is shown in Figure~\ref{co:lst:sum}.  This algorithm
cannot be discovered easily using the method outlined in this thesis, and a
way to extend the framework to optimize programs as creatively as humans still
eludes us at the moment.  Higham~\etal~\cite{higham02} discuss in great depth
many existing numerical accuracy problems encountered in finite-precision
computation of polynomials and linear algebra subprograms and how to analyze
and overcome inaccuracies, often in terms of relative errors.  Bridging the
gap between computational and mathematical approaches for numerical analysis
will allow us to automate many accuracy optimizations that were previously
unexplored by the tool.
\begin{figure}[ht]
    \centering
\begin{lstlisting}[]
    float compensated_summation(float X[N])
    {
        float sum = 0.0f;
        float e = 0.0f;
        for (i = 0; i < n; i++)
        {
            float tmp = sum;
            float y = X[i] + e;
            sum = tmp + y;
            e = (temp - sum) + y;
        }
        return sum;
    }
\end{lstlisting}
    \caption{%
        Kahan's compensated summation algorithm to accurately compute the sum
        of $n$ elements $\sum_{i = 0}^{n-1} x_i$.
    }\label{co:lst:sum}
\end{figure}

\textbf{Continuity analysis and optimization.} The robustness of programs
are very important to us.  In many cases, we wish our algorithms to be
free from discontinuity, \ie~a small change in the initial condition
would not result in an undesirably large jump in the outputs.  For this,
Chaudhuri~\etal~\cite{chaudhuri11} and Goubault~\etal~\cite{goubault13}
respectively propose methods to analyze the robustness of programs.  The
former approach formally proves whether an algorithm is ill-conditioned in
terms of the existence of discontinuity, whereas the latter statically analyze
programs to determine if round-off errors introduce significant discontinuous
behaviour.  To illustrate, consider an \iflit~branch,
``\lstinline[basicstyle=\tt]{if ($e$ > 0) $c_1$ else $c_2$}'', where $e$
is a floating-point expression.  When $e$ is positive and very close to
$0$ when evaluated in real arithmetic, the floating-point result of $e$
could be non-positive, due to the effects of the round-off errors.  In
these extraordinary cases, the $c_2$ branch may be executed instead of the
intended $c_1$.  These above new techniques could inspire us to implement the
optimization of discontinuous behaviour, such as the one shown in the example,
as another objective.

\textbf{Memory partitioning.} The experimental results in this work see a
diminishing performance return when loops are deeply unrolled, because of a
memory bottleneck.  As memory accesses saturate in loop execution, \ie~all
memory ports are working in 100\% utilization, it is unable to gain further
performance improvements.  Currently the tool stops exploring further loop
unrolling when this happens.  By automatically partition arrays upon hitting
such a memory bottleneck, further throughput improvements can be achieved.

\textbf{Other practical considerations.}  Finally, we may consider design
perspectives that could make the resulting tool much more usable.  For
instance, programs may still be optimized by not having any knowledge about the
input variables.  Herbie~\cite{panchekha15} makes no assumption about the input
space, and can nevertheless optimize arithmetic expressions, by splitting the
input space into regimes.


% \section{Tool Usage}
% \label{cc:sec:usage}

% \soap~is a source-to-source optimizer that specifically targets numerical
% program statements written in a subset of standard C99.  The tool supports
% arithmetic and Boolean expressions, assignment statements, \iflit{} statements,
% \whilelit{} loops and \forlit{} loops.  The numerical data types we allow are
% $\inttype$ and $\floattype$, as well as single- and multi-dimensional array
% types.

% The program below is an example usage of \soap~in a C program.  Note that it
% specifies the input values are respectively a two-dimensional array \verb|A|,
% where its elements are single-precision floating point values between 0 and 1,
% and an integer \verb|T| equals to $20$.  It also indicates the only output that
% we care from this code is the resultant \verb|A|.
% \begin{lstlisting}
  % #define N 1024
  % #pragma soap begin
  % #pragma soap in float A[N][N]=[0,1], int T=20
  % #pragma soap out A
  % for (int t = 0; t < T; t++)
    % for (int i = 1; i < N-1; i++)
      % for (int j = 1; j < N-1; j++)
        % A[i][j] = 0.2f * (A[i-1][j] +
          % A[i][j-1] + A[i][j] +
          % A[i][j+1] + A[i+1][j]);
  % #pragma soap end
% \end{lstlisting}

% \soap~is an open-source command-line utility, which only requires the user
% to provide a program written in C extended with the above \verb|#pragma|
% statements.  The Pareto optimal programs are all automatically generated by
% \soap, each is accompanied with our estimations of its latency and resource
% usage, and an analyzed bound on round-off errors.  These programs can then be
% given to \gls{vhls} to be synthesized into circuits.


\section{Final Remarks}
\label{cc:sec:final_remarks}

This thesis adapts existing techniques such as accuracy, latency and resource
usage analysis, and further introduces novel approaches, \eg~\gls{mir} and
efficient equivalence discovery, and delivers them in a unified framework.
The functional equivalence relaxation paradigm is relatively under-explored,
because these optimizations are often highlighted as \emph{unsafe} by the
\gls{hls} tools, as they cannot analyze the numerical implications of these
optimizations.  \Gls{hls} tools therefore have very limited optimization
options base on this particular concept.  With the constructive results
produced by this thesis, optimizations based on our concept can not only raise
performance measures, but also result in even \emph{safer} implementations
as we improve numerical accuracies.  The equivalence discovery algorithm in
tandem with \glspl{mir} could have great potential in compiler optimization
based on our concept.  Furthermore, since machine learning algorithms are
error-resilient~\cite{lesser11, kim09, holt91, zhu03}, the methods demonstrated
in this thesis have promising capabilities to improve the resource usage,
latency and accuracy of them.
