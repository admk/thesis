\section{Program Analysis and Abstract Interpretation}
\label{bg:sec:abstract_interpretation}

As our way of living is becoming increasingly dependent on programs,
errors in safety-critical system can incur huge expenses, and even cost
lives.  For example, the maiden flight of Ariane 5 resulted in a failure,
because of a software instruction failed to convert a 64-bit floating-point
number into 16-bit signed integer, as the result was too large to be
represented~\cite{dowson97}.  The Patriot defense system failed to intercept
an incoming missile because of an accumulated round-off error in the
system's internal clock, which resulted in the deaths of 28 people in
1991~\cite{patriot}. \emph{Static analysis}, a process of analyzing a piece of
program written in an \gls{hll} without executing it, is therefore a research
topic of great importance to prevent similar catastrophic errors and mitigate
the cost of failure in the future.

It is unfortunate that because of the halting problem~\cite{turing37} and a
direct consequence of it, Rice's theorem~\cite{rice53}, any nontrivial property
on the outcome of a program is in general undecidable.  This means that an
interesting property, a \emph{yes} or \emph{no} question which is never always
true or always false for all programs, is in general \emph{undecidable};
or in other words, it cannot be answered algorithmically.  Even a question
as seemingly innocuous as \emph{``does this program return zero''} falls
into this category.  A static analyzer, when faced with such a question,
therefore do not attempt to produce a definite \emph{yes} or \emph{no},
instead it answers with either a definite \emph{yes} or \emph{I don't
know}~\cite{mine04}; and producing a meaningful \emph{yes} in an efficient
manner poses a challenging task to static analyzers.  Additionally, they
often rely heavily on formal techniques to perform well.  Typical techniques
employed include symbolic execution, model checking~\cite{kroening03},
satisfiability modulo theories~\cite{demoura08}, data-flow analysis based on
lattices~\cite{nielson99}, abstract interpretation~\cite{cousot77}, \etc{}

There are static analyzers specifically tailored to prove the absence of
run-time errors in computing systems.  Astr\'ee~\cite{astree}, based on
the theory of abstract interpretation, has been successful in proving the
safety of the flight control systems of the Airbus A340 and A380 series,
and the automatic docking software of the Jules Vernes Automated Transfer
Vehicle~\cite{dasia09}.  Other static analysis tools which employ abstract
interpretation include MathWorks's Polyspace Bug Finder~\cite{polyspace},
Fluctuat~\cite{Fluctuat}, and ECLAIR~\cite{eclair}.

This section starts by introducing the data-flow analysis framework to analyze
a simple program, abstract interpretation is then applied to this example, and
the properties of the resulting analysis are further discussed.

% This is then extended to define a scalable analysis capturing accuracy.
% Later in Chapter~\ref{chp:progopt} we accommodate sequential statements,
% \iflit~branches and \whilelit~loops in the accuracy analysis, and in
% Chapter~\ref{chp:latopt}, we further improve our analysis by supporting
% multi-dimensional arrays.

\subsection{Data-Flow Analysis Framework}
\label{bg:sub:data_flow}

In this section, we use the \gls{dfa} framework~\cite{nielson99} to deduce
the semantics of a program named \verb|simple| in Figure~\ref{bg:lst:simple},
which consists of only one variable $\varx$.  We assume an initial set $\iota
\subseteq \realset$ of values of $\varx$, and the property pertaining to
us is whether a particular value $\varx_\mathrm{invalid}$ is unreachable.
Computations are performed in real arithmetic for simplicity.  By computing an
$X$, the set of all reachable final values of $\varx$, it suffices to check
$\varx_\mathrm{invalid} \notin X$.  A sensible definition for the set of
values can be reached by $\varx$ is a subset of all real numbers $\realset$,
\ie~an element of $\powersetof\realset$, where $\powersetof\realset$ denotes
the \emph{power set} of $\realset$, also known as the set of all subsets of
$\realset$.
\begin{figure}[ht]
    \begin{lstlisting}
        real simple(real x) {
            while (x > 1.0)
                x *= 0.9;
            return x;
        }
    \end{lstlisting}
    \caption{%
        A simple program example to be statically analyzed.
    }\label{bg:lst:simple}
\end{figure}

The first step of \gls{dfa} is to translate the body of \verb|simple| into a
\gls{cdfg}, as shown in Figure~\ref{bg:fig:cdfg} where each block consists of
a single statement or conditional, and the edges in the graph model the data-
and control-flows.  The \textbf{tt} and \textbf{ff} respectively highlight the
control-flow branches taken when the conditional \mbox{``\texttt{x < 1}''}
evaluates to either true or false.
\begin{figure}[ht]
    \centering
    \tikzstyle{sblock} = [
        draw, rectangle, minimum height=2em, minimum width=4em
    ]
    \begin{tikzpicture}[node distance=4em]
        \node(entr) {\textbf{entry}};
        \node(cond) [sblock, below of=entr] {\texttt{x > 1}};
        \node(stmt) [sblock, below of=cond] {\texttt{x = 0.9 * x;}};
        \node(midl) [coordinate, left of=stmt, xshift=-1.5em] {};
        \node(midr) [coordinate, right of=stmt, xshift=1.5em] {};
        \node(rtrn) [coordinate, below of=stmt, yshift=1em]
            {\texttt{return x;}};
        \node(exit) [below of=rtrn, yshift=1em] {\textbf{exit}};
        \draw[->] (entr) -- node[auto]{0} (cond);
        \draw[->] (cond) -- node[right]{\textbf{tt}} node[left]{1} (stmt);
        \draw[- ] (stmt) -- (midl);
        \draw[->] (midl) |- node[auto]{2} (cond);
        % \draw[->] (stmt) to[out=180, in=180] (cond);
        \draw[- ] (cond) -| node[auto, near start]{\textbf{ff}} (midr);
        \draw[- ] (midr) |- node[auto]{3} (rtrn);
        % \draw[->] (cond) to[out=0, in=0] (rtrn);
        \draw[->] (rtrn) -- (exit);
    \end{tikzpicture}
    \caption{%
        The \acrshort{cdfg} of \texttt{simple} in Figure~\ref{bg:lst:simple}.
    }\label{bg:fig:cdfg}
\end{figure}

The individual blocks in the \gls{cdfg} can therefore be defined as functions
$f: \powersetof\realset \to \powersetof\realset$, where both its input and
output are elements of $\powersetof\realset$.  For instance, for the statement
``\lstinline[basicstyle=\tt]{x *= 0.9;}'' a function $f_1$ can be defined as
follows:
\begin{equation}
    f_1(S) = \{ 0.9 v \mid v \in S \}.
\end{equation}
Here, the definition of $f_1$ indicates that for all possible input values $v$
of $\varx$ in the set $S$, we multiply it by $0.9$ and collect the multiplied
results into a new set as the output of $f_1$.  Similarly, because ``\texttt{x
> 1}'' has two conditional branches, two functions, $f_{2,\truelit}$ and $f_{2,
\falselit}$, respectively for both true- and false-branches of it can be
defined:
\begin{equation}
    \begin{aligned}
        f_{2, \truelit}(S) &= S \cap \{ v \in \realset \mid v > 1 \}, \\
        f_{2, \falselit}(S) &= S \cap \{ v \in \realset \mid v \leq 1 \}.
    \end{aligned}
\end{equation}
where $X \cap Y$ computes the intersection of the two sets $X$ and $Y$.

In the next step, the edges of the \gls{cdfg} are labelled with numbers 0, 1,
2 and 3 to signify different locations of the program.  For each edge labelled
$i$, it is now possible to compute an $A(i)$, a set of values that could be
reached by $\varx$ in a program execution at each location $i$, by wiring up
the functions $f_1$, $f_{2, \truelit}$ and $f_{2, \falselit}$ that correspond
to program statements.  This gives rise to the following system of data-flow
equations:
\begin{align}
    A(0) &= \iota,
        \label{bg:eq:dfa0} \\
    A(1) &= f_{2, \truelit}(A(0) \cup A(2)),
        \label{bg:eq:dfa1} \\
    A(2) &= f_1(A(1)),
        \label{bg:eq:dfa2} \\
    A(3) &= f_{2, \falselit}(A(0) \cup A(2)),
        \label{bg:eq:dfa3}
\end{align}
where $A(0) \cup A(1)$ is the union of $A(0)$ and $A(1)$.

Unfortunately, computationally solving this system of equations is not an
easy task.  In the rest of this section, the two significant impediments
are explained, and subsequently, theories are introduced to address them.
Section~\ref{bg:sub:lfp} discusses how the system of data-flow equations can
be solved analytically for the most desired solution, using a minimal set of
reachable values for $A(1)$ as an example.  However, this analytical solution
cannot be computed by a machine.  By building on top of this foundation,
Section~\ref{bg:sub:intervals} therefore provides a practical solution to
compute a safe approximation of this set by an algorithm.


\subsection{Least Fixpoint Solution to a Data-Flow Analysis Problem}
\label{bg:sub:lfp}

There are multiple solutions to this system.  For example, we can solve
it manually by substituting $A(0)$ and $A(2)$ in~\eqref{bg:eq:dfa1}
with~\eqref{bg:eq:dfa0} and~\eqref{bg:eq:dfa2}.  We arrive at:
\begin{equation}
    A(1) = \left(
        \iota \cup \left\{ 0.9 v \mid v \in A(1) \right\}
    \right) \cap \{ v \in \realset \mid v > 1 \}.
    \label{bg:eq:dfa_a1}
\end{equation}
It turns out that the set of all real numbers greater than $1$:
\begin{equation}
    A(1) = \{ v \in \realset \mid v > 1 \},
    \label{bg:eq:a11}
\end{equation}
is a solution to~\eqref{bg:eq:dfa_a1}.  Substituting $A(1)$ in the right-hand
side of~\eqref{bg:eq:dfa_a1} with this value proves that it is indeed the
solution for this equation, assuming all sets below are subsets of $\realset$
to simplify the derivation:
\begin{equation}
    \begin{aligned}
        A(1)
        &= \bigg( \iota \cup \Big\{ 0.9 v \mid v \in
                \left\{ v^\prime \mid v^\prime > 1 \right\}
           \Big\} \bigg) \cap \{ v \mid v > 1 \} \\
        &= \bigg( \iota \cup \left\{ 0.9 v \mid v > 1 \right\} \bigg) \cap
           \{ v \mid v > 1 \}
         = \bigg( \iota \cup \left\{ v \mid v > 0.9 \right\} \bigg) \cap
           \{ v \mid v > 1 \} \\
        &= \bigg( \iota \cap \{ v \mid v > 1 \} \bigg) \cup
           \bigg(
               \left\{ v \mid v > 0.9 \right\} \cap \{ v  \mid v > 1 \}
           \bigg) \\
        &= \bigg( \iota \cap \{ v \mid v > 1 \} \bigg) \cup
           \{ v \mid v > 1 \}
         = \{ v \mid v > 1 \}.
    \end{aligned}
\end{equation}

Intuitively, a manual inspection of \verb|simple| finds that $\varx$ can
reach values $v$, $0.9 v$, $0.9^2 v$, and so on, such that all values in this
sequence are greater than $1$, for each $v \in \iota$; or more succinctly, an
alternative solution to $A(1)$ should be:
\begin{equation}
    A(1) = \{ v^\prime \mid
        v^\prime > 1 \wedge
        v^\prime = 0.9^k v \wedge
        v \in \iota \wedge
        k \in \naturalset
    \},
    \label{bg:eq:a12}
\end{equation}
where $k \in \naturalset$ denotes $k$ is one of $0, 1, 2, \mathellipsis$, \ie~a
natural number.

It is evident to us the latter solution~\eqref{bg:eq:a12} is more precise,
hence more desirable, than the former~\eqref{bg:eq:a11}.  Not only does it
contain information the former has, \ie~all values reachable by $A(1)$ is
greater than 1, it also expresses the fact that it only consists of values
of the form $0.9^k v$ that are greater than $1$, where $v \in \iota$ and $k
\in \naturalset$.  A useful definition of preciseness is therefore the subset
relation ``$\subseteq$''.  If it is known that $X \subseteq X^\prime$, and $X$
and $X^\prime$ are both solution to a system of data-flow equations, then $X$
is clearly more appealing than $X^\prime$.

The set $\powersetof\realset$, with a preciseness ordering ``$\subseteq$'',
is a \emph{partially ordered set}.  It has three following properties for any
$X, Y, Z \in \powersetof\realset$: it is \emph{reflexive}, $X \subseteq X$; it
has the \emph{antisymmetry} property, \ie~if $X \subseteq Y$ and $Y \subseteq
X$, then $X = Y$; and finally it is transitive, if $X \subseteq Y$ and $Y
\subseteq Z$, then $X \subseteq Y$.  In contrast to a total order such as the
set of reals $\realset$, not every two elements in $\powersetof\realset$ can be
compared, \eg~neither of the sets $\{1, 2, 3\}$ and $\{2, 3, 4\}$ is a subset
of one another.

For the purpose of computing the solution to $A(1)$'s
equation~\eqref{bg:eq:dfa_a1}, a function $f: \powersetof\realset \to
\powersetof\realset$ can be defined:
\begin{equation}
    f(X) = \left(
        \iota \cup \left\{ 0.9 v \mid v \in X \right\}
    \right) \cap \{ v \in \realset \mid v > 1 \},
    \label{bg:eq:a1_transfer}
\end{equation}
so that all solutions of the original equation~\eqref{bg:eq:dfa_a1} are now in
this following set, which are known as the \emph{fixpoints}\footnotemark[1] of
$f$:
\begin{equation}
    \mathrm{Fix}(f) = \left\{
        X \in \powersetof\realset \mid
        f(X) = X
    \right\}.
\end{equation}
\footnotetext[1]{%
    Another common name for \emph{fixpoint} is \emph{fixed point}.  To avoid
    being mistaken for the fixed point representation, a binary number
    representation, the term \emph{fixpoint} is used instead.
}

By using this particular definition of preciseness, two important questions
arise:
\begin{enumerate}

    \item Is the most precise solution unique?  A unique most precise solution
    is defined as the only one which is the most precise among all possible
    solutions to the systems of data-flow equations.  In other words, if it
    exists, then it is defined as the \emph{least fixpoint} (\gls{lfp}) of
    $f$ which is a subset of all other fixpoints, \ie~$\lfp (f) \subseteq Y$
    for any $Y \in \mathrm{Fix}(f)$.  As we have discussed earlier, multiple
    fixpoints exist, and it is possible that these fixpoint solutions are not
    comparable.

    \item If a unique solution exists and it is unique, how do we find it?
    This is equivalent to finding a way to compute the \gls{lfp} $\lfp(f)$
    using $f$.

\end{enumerate}

As it turns out, to answer the first question, Tarski's fixpoint
theorem~\cite{tarski55, nielson99} can be used to prove that $\lfp (f)$ is
indeed unique.

For the second question, Kleene's fixpoint theorem shows that in our example
analysis, the most precise solution of $A(1)$ can be computed using:
\begin{equation}
    \lfp (f) = \bigcup_{k \in \naturalset} f^k (\varnothing),
\end{equation}
where a function of the form $h^n(x)$, where $h: \mathsf{M} \to \mathsf{M}$ for
any domain $\mathsf{M}$ and $n \in \naturalset$, is recursively defined as:
\begin{equation}
    h^n(x) = \left\{
        \begin{aligned}
            & h(h^{n-1}(x)) \quad && \text{if~} n > 0, \\
            & x && \text{if~} n = 0.
        \end{aligned}
    \right.
\end{equation}
The functions $f^k(\varnothing)$ for the first $k+1$ iterations can be
evaluated as follows:
\begin{equation}
    \begin{aligned}
        f^0(\varnothing) &= \varnothing, \quad\quad
        f^1(\varnothing) = \iota \cap \{ v \mid v > 1 \}, \\
        f^2(\varnothing) &= f(f^1(\varnothing))
               = \left(
                     \iota \cup
                     \{ 0.9v \mid v \in \iota \}
                 \right) \cap \{ v \mid v > 1 \}, \\
        f^3(\varnothing) &= \left(
                     \iota \cup
                     \{ 0.9v \mid v \in \iota \} \cup
                     \{ 0.9^2 v \mid v \in \iota \}
                 \right) \cap \{ v \mid v > 1 \}, \mathellipsis, \\
        f^k(\varnothing) &= \left(
                     \iota \cup
                     \{ 0.9v \mid v \in \iota \} \cup
                     \mathellipsis \cup
                     \{ 0.9^{k-1} v \mid v \in \iota \}
                 \right) \cap \{ v \mid v > 1 \}.
    \end{aligned}
\end{equation}
Finally, the most precise solution to~\eqref{bg:eq:dfa_a1} can be computed
using the \gls{lfp} formula for $f$, which is exactly the same as the
alternative solution that was manually computed in~\eqref{bg:eq:a12}:
\begin{equation}
    \begin{aligned}
        \lfp (f)
            &= \bigcup_{k \in \naturalset} f^k (\varnothing)
             = \{ v \mid v > 1 \} \cap
               \bigcup_{k \in \naturalset} \{ 0.9^k v \mid v \in \iota \} \\
            &= \{
               v^\prime \mid
               v^\prime > 1 \wedge v^\prime = 0.9^k v \wedge
               v \in \iota \wedge k \in \naturalset
            \}.
    \end{aligned}
\end{equation}

Even though we have derived a method to statically analyze a program,
significant obstacles stymie the efficient usage of it.  Firstly, in the case
study of \verb|simple|, because the \gls{lfp} is evaluated as the union of
$f^k(\varnothing)$ in a sequence, this sequence is likely to be infinite, and
thus cannot be computed fully.  Secondly, the set of input values, $\iota$,
not only determines the number of iterations necessary in order to calculate
the \gls{lfp}, but also impacts the amount of computation required in each
iteration.  For instance if $\iota = \{4\}$ then it is only necessary to
track the computation for a single input value $4$, whereas when $\iota = \{
v \mid 0 \leq v \leq 1000 \}$, there are infinitely many values in the set.
As a result, in general, the \gls{lfp} of an arbitrary self-map function $f:
\mathsf{L} \to \mathsf{L}$ is thus not computable in finite amount of time.  In
Section~\ref{bg:sub:intervals}, a method known as abstract interpretation is
introduced to overcome the computability problem.


\subsection{Abstract Interpretation with Intervals}
\label{bg:sub:intervals}

A framework of methods, known as \gls{ai}, is proposed by Cousot
\etal~\cite{cousot77} to formally mitigate the problem of computability
in program analysis.  Instead of finding the \gls{lfp}, which may not be
computable, it is much more efficient to work out an \emph{approximation} of
the \gls{lfp}\@.  Despite the outcome of an \gls{ai}-based static analysis not
being as precise as the \gls{lfp}, the significant benefits of \gls{ai} is
two-fold.  Firstly, the program analysis framework can now produce a ``yes'' or
``I don't know'' answer to a query of program property in a finite amount of
time.  Secondly, it provides the means to prove the correctness of an answer
produced by the static analyzer using \gls{ai} in formal mathematics.

Here a simple formulation of \acrlong{ia} is first introduced, which is then
exercised in the \gls{ai} framework, again by using the \verb|simple| example
in Figure~\ref{bg:lst:simple}.

\subsubsection{Interval Arithmetic}
\label{bg:ssub:interval}

\Gls{ia}, is a method to enclose a set of solutions that may arise in
computation problems~\cite{moore}.  The standard method is to use a \emph{pair}
of values $\interval{a}{b}$, to represent $\{ v \mid a \leq v \leq b \}$, a
potentially \emph{infinite} set of real numbers between $a$ and $b$.  As it
is closely related to sets of reals and real arithmetic, \gls{ia} gets the
benefits of both worlds.

Firstly, operations, similar to the set operations such as the subset relation
``$\subseteq$'', union ``$\cup$'' and intersection ``$\cap$'' from $\realset$,
can also be defined for real intervals $\intervalset$.  The corresponding
operations are known as the partial ordering ``$\sqsubseteq$'', \emph{join}
``$\join$'' and \emph{meet} ``$\meet$'', a partially ordered set with these
operations defined for all elements within it is a complete lattice.  The
definitions of these operators on two intervals $\interval{a}{b}$ and
$\interval{c}{d}$ are as follows:
\begin{equation}
    \arraycolsep=0.5ex
    \begin{array}{lcll}
        \interval{a}{b} & \sqsubseteq & \interval{c}{d}
            &\defeq a \geq c \wedge b \leq d, \\
        \interval{a}{b} & \join & \interval{c}{d}
            &\defeq [\min(a, c), \max(b, d)], \\
        \interval{a}{b} & \meet & \interval{c}{d}
            &\defeq \left\{
                \begin{aligned}
                    & [\max(a, c), \min(b, d)] \quad &&
                        \text{if~} \max(a, c) \leq \min(b, d), \\
                    & \bot && \text{otherwise}.
                \end{aligned}
            \right.
    \end{array}\label{bg:eq:interval_relations}
\end{equation}
where $s \defeq t$ indicates $s$ is defined as $t$, $\min(x, y)$ and $\max(x,
y)$ respectively compute the minimum and maximum of $x$ and $y$, and $\bot$,
$\top$ respectively denote intervals with no elements, \ie~an empty interval,
and the entire set of reals.  For completeness, the above relations can be
further extended for $\top$ and $\bot$, where $X^\sharp \in \intervalset$ is an
interval:
\begin{equation}
    \begin{aligned}
        X^\sharp \join \top &\defeq \top, \quad \text{and} \quad
        X^\sharp \meet \bot &\defeq \bot.
    \end{aligned}
\end{equation}
As a consequence, $\bot$ and $\top$ are respectively the least and greatest
elements of $\intervalset$.  This means that $\bot \sqsubseteq X^\sharp$ and
$X^\sharp \sqsubseteq \top$ for any interval $X^\sharp$.

Secondly, in a similar fashion to real arithmetic, arithmetic operations can
also be defined for real intervals:
\begin{equation}
    \begin{aligned}
        \interval{a}{b} + \interval{c}{d} &= \interval{a + c}{b + d}, \\
        \interval{a}{b} - \interval{c}{d} &= \interval{a - d}{b - c}, \\
        -\interval{a}{b} &= \interval{-b}{-a}, \\
        \interval{a}{b} \times \interval{c}{d}
            &= \interval{\min(s)}{\max(s)}, \\
        \text{where~} s &= \left\{
            a \times c, a \times d, b \times c, b \times d
        \right\}.
    \end{aligned}
    \label{bg:eq:interval_operations}
\end{equation}
A scalar value $x$, which can be abbreviated by $x$ itself, represents an
interval $\interval{x}{x}$ in \gls{ia}.

\subsubsection{An Informal Approach to Approximation}
\label{bg:ssub:informal}

\Gls{ai} is a theoretical framework to approximate mathematical objects
that are not directly computable.  We start by explaining what it means to
\emph{approximate}, then show how an approximation can be proved to be safe.

In the \gls{dfa} of \verb|simple| in Section~\ref{bg:sub:data_flow}, we
arrived at a function $f: \powersetof\realset \to \powersetof\realset$ defined
in~\eqref{bg:eq:a1_transfer}, which is not directly computable.  The \gls{ia}
introduced earlier could serve as an inspiration to produce a different
function $\tilde{f}: \intervalset \to \intervalset$, which is very similar to
the original $f$:
\begin{equation}
    \tilde{f}(X^\sharp) = \left(
        \iota^\sharp \join \left( 0.9 X^\sharp \right)
    \right) \meet [1, \infty],
    \label{bg:eq:a1_transfer_interval}
\end{equation}
where $\iota^\sharp$ is an interval that bounds the set of initial values
$\iota$.  There are two noticeable differences between $f$ and $\tilde{f}$.
Firstly, the domain, where $\tilde{f}$ carries out its computation, is
$\intervalset$ instead of $\powersetof\realset$.  Secondly, because the domain
used is different from $f$, the function definition is therefore updated
accordingly for $\tilde{f}$.

When given an input $X$, $f$ must enumerate on all $X$ to compute the result
$f(X)$ precisely.  As we have discussed earlier this is infeasible as $X$
could contain infinite number of values.  Conversely, $\tilde{f}$ does not
suffer from this problem, since~\eqref{bg:eq:interval_operations} dictates any
\gls{ia} operations can be performed by \emph{finite} number of real arithmetic
computations.

As it is possible to prove that both fixpoint theorems in
Section~\ref{bg:sub:lfp} hold for $\intervalset$ and $\tilde{f}$, its
\gls{lfp}, an approximation to the \gls{lfp} to $f$, can therefore be computed
as follows:
\begin{equation}
    \lfp (\tilde{f}) = \bigsqcup_{k \in \naturalset} \tilde{f}^k (\bot).
\end{equation}
For example, consider the case when $\iota = [0, 10]$,
\begin{equation}
    \begin{aligned}
        \tilde{f}^0 (\bot) &= \bot, \\
        \tilde{f}^1 (\bot) &= \left(
                [0, 10] \join 0.9 \bot
            \right) \meet [1, \infty] = [1, 10], \\
        \tilde{f}^2 (\bot) &= \left(
                [0, 10] \join \left( 0.9 \times [1, 10] \right)
            \right) \meet [1, \infty] = [1, 10], \mathellipsis
    \end{aligned}
\end{equation}
As all other values in the sequence evaluates to $[1, 10]$, $\lfp(\tilde{f})$
is hence $[1, 10]$, which was computed in 3 iterations.  It is easy to see
in Figure~\ref{bg:lst:simple} that the reachable values of $\varx$ before
executing the statement ``\verb|x *= 0.9;|'' is indeed $[1, 10]$.

In many cases, an algorithm to compute $\tilde{f}^k(\bot)$ can terminate.
It is clear that if $\tilde{f}^k(\bot) = \tilde{f}^{k+1}(\bot)$ for some
$k \in \naturalset$, then for all integers $j$ that is greater than $k$,
$\tilde{f}^{j}(\bot) = \tilde{f}^{k}(\bot)$ and there is no point in computing
$\tilde{f}^{j}(\bot)$.  Widening and narrowing operators~\cite{cousot77,
nielson99} can be used to reduce the number of iterations required to reach
a fixpoint, hence accelerating, or even ensuring, termination by sacrificing
precision of the computed fixpoint, \ie~the result is no longer the \gls{lfp},
but is a fixpoint $X \sqsupseteq \lfp(\tilde{f})$ that can be computed more
easily.

\subsubsection{Galois Connection}
\label{bg:ssub:galois}

Although our informal derivation gives us empirical evidence of the usefulness
and correctness of intervals in place of real sets, a series of questions
pertaining the theory behind it remain.  The questions are of the format
``\emph{is $X$ a correct abstraction of $Y$}'', where $X$ and $Y$ refer to each
of the following pairs: $(\intervalset, \powersetof\realset)$, $(\tilde{f},
f)$, and $(\lfp (\tilde{f}), \lfp (f))$.

Fortunately, Cousot~\etal~\cite{cousot77} show that if a \emph{Galois
connection} can be formed between $\powersetof\realset$ and $\intervalset$,
then all of the above questions can be now answered with a resounding
``\emph{yes}''.  We can define~\cite{nielson99}:
\begin{definition}
    \textup{[Galois connection]}
    A Galois connection is a relation between two complete lattices
    $\lattice{\mathsf{L}}{\subseteq}$ and $\lattice{\mathsf{M}}{\sqsubseteq}$,
    given by a pair of functions $\alpha: \mathsf{L} \to \mathsf{M}$ and
    $\gamma: \mathsf{M} \to \mathsf{L}$, such that for all $l \in \mathsf{L}$
    and $m \in \mathsf{M}$:
    \begin{equation}
        \alpha (l) \sqsubseteq m
        \quad
        \text{~if and only if~}
        \quad
        l \subseteq \gamma (m),
        \label{bg:eq:galois}
    \end{equation}
    alternatively the following property may sometimes be easier to work with:
    \begin{equation}
        l \subseteq \gamma(\alpha(l))
        \quad
        \text{~and~}
        \quad
        \alpha(\gamma(m)) \sqsubseteq m.
        \label{bg:eq:galois_alt}
    \end{equation}
\end{definition}\vspace{-16.5pt}
The Galois connection above can often be concisely written as:
\begin{equation}
    \lattice{\mathsf{L}}{\subseteq}
        \galois{\alpha}{\gamma}
    \lattice{\mathsf{M}}{\sqsubseteq}.
\end{equation}
Here, the functions $\alpha$ and $\gamma$ are often called the
\emph{abstraction} function and \emph{concretization} function respectively.

Bearing on the informal approach earlier, the following Galois connection can
be established to formalize it:
\begin{equation}
    \lattice{\powersetof\realset}{\subseteq}
    \galois{\alpha}{\gamma}
    \lattice{\intervalset}{\sqsubseteq},
\end{equation}
The functions $\alpha: \powersetof\realset \to \intervalset$ and $\gamma:
\intervalset \to \powersetof\realset$ that satisfy~\eqref{bg:eq:galois} can be
defined as follows:
\begin{equation}
    \begin{aligned}
        \alpha(X) &= \interval{\inf(X)}{\sup(X)}, \\
        \gamma\left( \interval{a}{b} \right) &= \left\{
            v \mid a \leq v \leq b
        \right\},
    \end{aligned}
\end{equation}
where $\inf(X)$ and $\sup(X)$ are respectively the infimum and supremum of $X$.
Here $\alpha$ takes a potentially infinite set of reals and translate it into
a pair of values representing an interval bounding the reals, whereas $\gamma$
performs the backwards translation from an interval to a set of reals.  For
instance:
\begin{equation}
    \begin{aligned}
        \alpha\left(\left\{1, 1.2, 3\right\}\right) &= \interval{1}{3}, \\
        \gamma\left(\interval{1}{3}\right) &= \left\{
            v \mid 1 \leq v \leq 3
        \right\}.
    \end{aligned}
\end{equation}
It is clear that information could be lost when the abstraction function
$\alpha$ is applied.  As shown in the example above, a set of three real values
$1$, $1.2$ and $3$ can be approximated by an interval $\interval{1}{3}$, which
represents a set of real values ranging from $1$ to $3$.

Furthermore, from a function $g: \mathsf{L} \to \mathsf{L}$, the \gls{ai}
framework allows an approximated function $g^\sharp: \mathsf{M} \to \mathsf{M}$
to be inductively abstracted by the Galois connection~\cite{nielson99}.  For
example, consider~\eqref{bg:eq:a1_transfer} which is not computable, an
approximated candidate $f^\sharp$ can be computed by $\alpha \circ f \circ
\gamma$.  Since we are computing in $\intervalset$, we assume that $\iota =
\gamma(\iota^\sharp)$ and use $\underline{a^\sharp}$ and $\overline{a^\sharp}$
respectively denote the lower- and upper-bounds of an interval $a^\sharp$,
\ie~$a^\sharp = \interval{\underline{a^\sharp}}{\overline{a^\sharp}}$:
\begin{equation}
    \begin{aligned}
        f^\sharp(X^\sharp)
        &= \alpha \left( f \left(
            \gamma \left( X^\sharp \right)
        \right) \right)
        = \alpha \left( f \left( \left\{
            v \mid \underline{X^\sharp} \leq v \leq \overline{X^\sharp}
        \right\} \right) \right) \\
        &= \alpha \left(
        \left(
            \iota \cup
            \left\{
                0.9 v \mid
                \underline{X^\sharp} \leq v \leq \overline{X^\sharp}
            \right\}
        \right) \cap \{ v \mid v > 1 \} \right) \\
        &= \alpha \left(
            \left( \iota \cap \{ v \mid v > 1 \} \right) \cup
            \left( \left\{
                v \mid
                0.9 \underline{X^\sharp} \leq v \leq 0.9 \overline{X^\sharp}
            \right\} \cap \{ v \mid v > 1 \} \right)
        \right).
    \end{aligned}
\end{equation}

We introduce $\epsilon$ to denote an infinitesimal positive value.  Because
$\alpha(A \cup B) = \alpha(A) \sqcup \alpha(B)$:
\begin{equation}
    \begin{aligned}
        f^\sharp(X^\sharp)
        ={} & \alpha \left( \left\{
            v \mid \underline{\iota^\sharp}
                \leq v \leq \overline{\iota^\sharp}
        \right\} \cap \{ v \mid v > 1 \} \right) \sqcup \\
        &
        \alpha \left( \left\{
            v \mid
            \min(0.9 \underline{X^\sharp}, 1 + \epsilon)
                \leq v \leq 0.9 \overline{X^\sharp}
        \right\} \right),
    \end{aligned}
\end{equation}
The term $\epsilon$ vanishes when the abstraction function $\alpha$ is applied:
\begin{equation}
    \begin{aligned}
        f^\sharp(X^\sharp)
        &= \alpha \left( \left\{
            v \mid
            \min(\underline{\iota^\sharp}, 1 + \epsilon)
                \leq v \leq \overline{\iota^\sharp}
        \right\} \right) \sqcup
        \left( \interval{
            \min(\underline{0.9X^\sharp}, 1)
          }{
            \overline{0.9X^\sharp}
          }
        \right) \\
        &= \left( \iota^\sharp \sqcap \interval{1}\infty \right) \sqcup
           \left( 0.9 X^\sharp \sqcap \interval{1}\infty \right)
         = \left( \iota^\sharp \sqcup 0.9 X^\sharp \right) \sqcap
           \interval{1}\infty.
    \end{aligned}
\end{equation}
which is identical to $\tilde{f}$ defined
in~\eqref{bg:eq:a1_transfer_interval}, therefore the correctness of $\tilde{f}$
is analytically proven.  Although we derived a special case using the
approximate function induction technique, in a more general fashion, this
method can be applied to the functions and operators in the \gls{dfa} of
reachable sets of reals.  Consequently, the \gls{dfa} of a general program
based on intervals can be induced.

\subsubsection{Further Generalization}

For simplicity, \verb|simple| is used as an example of \gls{dfa}\@.  However
unlike \verb|simple|, general programs can consist of more than one variable.
A \gls{dfa} for a program should therefore compute a reachable set of values
for each variable.  A typical way do so is to use a mapping which associates
each program variable with a value, which is defined as an element $\sigma \in
\Sigma$, where $\Sigma = \left[\varset \to \mathsf{L}\right]$ and $\mathsf{L}$
is a set of values.  For example, a single program state of \verb|simple|
could be $\sigma_0 \in \left[\varset \to \realset\right]$, and $\sigma_0 =
[\varx \mapsto 0]$, which denotes that the state $\sigma_0$ has only a variable
$\varx$, and $\varx$ is assigned a value $0$.  We could also have a state that
captures multiple program states, \eg~$\sigma^\sharp \in \left[\varset \to
\intervalset\right]$, where $\sigma^\sharp(\varx)$ could be an interval.

Galois connections can be compositionally constructed~\cite{nielson99}.  If
we know that $\powersetof\realset \galois{\alpha}{\gamma} \intervalset$, then
the following Galois connection between mappings can also be constructed:
\begin{equation}
    [\varset \to \powersetof\realset]
        \galois{\alpha^\prime}{\gamma^\prime}
    [\varset \to \intervalset],
\end{equation}
where $\alpha^\prime(f) = \alpha \circ f$, $\gamma^\prime(g) = \gamma \circ g$,
and a term of the form $s \circ t (x)$ denotes $s(t(x))$.


\subsection{Abstract Domains}
\label{bg:sub:abstract_domains}

The interval domain offers us an efficient way to enclose reachable values
of variables.  However, it is unable to capture the correlation among these
variables.  For instance, if we know that $\varx$ and $\vary$ are reals between
$0$ and $1$, and $\varx \leq \vary$, intervals cannot express the relation
$\varx \leq \vary$ and using bounds $\varx \in [0, 1]$ and $\vary \in [0, 1]$,
we evaluate the expression $(1 - \vary)\varx$ is bounded by $0$ and $1$.  In
contrast, if we were able to make use of the inequality $\varx \leq \vary$,
then it is possible to deduce $(1 - \vary) \varx \leq (1 - \varx) \varx \leq
\frac{1}{4}$, which yields a much tighter bound than using intervals alone.

In general, the design of abstract domains is a trade-off between how good
a approximation it can obtain, and how efficiently it can be computed.  For
example, we could imagine another abstract domain, $\signset$, which only
captures the signedness of variables, to also enclose a set of reachable
values.  Although it is even faster than intervals to compute, it sacrifices
the precision of the value bounds.  Ranging from the fastest to the most
expensive, a hierarchy of abstract domains for reals and floating-point
computations are proposed by various authors as discussed below.

A new abstract domain, $\mathbf{Octagon}$, is proposed by Min\'e~\cite{mine07},
to enclose values in a system of difference-bound inequalities, which are of
the form:
\begin{equation}
    \varx - \vary \leq c, \quad \text{or} \quad
    \pm \varx \leq c,
\end{equation}
where $\varx$ and $\vary$ are variables and $c$ is a constant.  This domain
is very efficient as a Floyd-Warshall algorithm~\cite{floyd62}, which runs in
$\bigo(n^3)$, where $n$ is the number of variables, can be used to compute
it~\cite{mine04}.  Although not as efficient as intervals, it is much more
expressive than intervals.  This is because intervals are equivalent to a
set of inequalities of the latter form, $\pm \varx \leq c$, but they cannot
represent the former which captures the correlations between variables, $\varx
- \vary \leq c$.

Based on \gls{aa}, Ghorbal~\etal~\cite{ghorbal09} propose an abstract domain,
Taylor1+\@.  This domain uses an equation of the form to capture the bound on a
variable $\varx$:
\begin{equation}
    \tilde{\varx} =
        \alpha^\varx_0 + \sum_{i = 1}^{n} \alpha^\varx_i \varepsilon_i,
\end{equation}
where $\alpha^\varx_i \in \realset$ for each $i$, and each $\varepsilon_i$
known as the noise symbol, which has an unknown quantity that lies in $[-1,
1]$, introduces a perturbation weighted by $\alpha^\varx_i$ on the constant
$\alpha^\varx_0$.  If two variables $\varx$ and $\vary$ has correlation,
$\tilde{\varx}$ and $\tilde{\vary}$ can consist of the same $\varepsilon_i$ to
encode the correlation.  They found that for a $2^\mathrm{nd}$ order \gls{iir}
filter, Taylor1+ can give an exact bound on the filter output, whereas both
$\intervalset$ and $\mathbf{Octagon}$ failed as the bound size increase
exponentially in each iteration, albeit slower than both methods.

Cousot~\etal~\cite{cousot78} introduce the polyhedra approach to abstract
domains.  This method makes use of arbitrary polyhedra, which could be
expressed by a set of linear inequalities such as the following:
\begin{equation}
    \left\{
    \begin{aligned}
        & 2\varx + 7\vary \leq 32, \\
        & 8\varx - 3\vary + \varz \geq 0.
    \end{aligned}
    \right.
\end{equation}
Libraries that implement the polyhedra domain include APRON~\cite{apron} and
Parma Polyhedra Library~\cite{ppl}.  Although the polyhedra domain is very
accurate for linear computations, Ghorbal~\etal~\cite{ghorbal09} find both
libraries to be much slower than their \gls{aa} based approach.

In addition to abstract domains on reals and floating-point values,
integer-based domains are further presented by several authors.
Granger~\cite{granger89} introduce the simple congruences on integers,
which consists of equations of the form $\varx = a \mod b$, where $\varx$
is a variable and $a$ and $b$ are integers.  They further propose a linear
congruences formulation which is more expressive, for instance, an equation $3
\varx + 4 \vary = 5 \mod 6$ can be used to capture the relation between integer
variables $\varx$ and $\vary$.

\subsubsection{Round-off Error Analysis}
\label{bg:ssub:accuracy}

% The above abstract domains are all based on the standard semantics of programs,
% \ie~they all assume that the program accepts as inputs and computes as
% outputs numerical values.  Here we extend this notion to non-standard program
% semantics.  Alongside the analysis of reachable values, this technique further
% enables the analysis of numerical round-off errors in the execution of
% numerical algorithms.

In Section~\ref{bg:sec:discovering_equivalent_programs}, we will discuss
techniques which optimize numerical accuracies in a program by restructuring
it.  Their methods utilize a common analysis of floating-point round-off
errors, which is based on a formulation of \gls{ai}\@.  In this section, this
numerical accuracy analysis approach is thus explained in detail.

Because of the finite characteristic of IEEE 754 floating-point
format~\cite{ieee754}, it is not always possible to represent exact values
with it.  Computations in floating-point arithmetic often induce round-off
errors.  Therefore, Martel~\cite{martel07} bound the ranges of the values
in floating-point calculations, as well as the round-off errors introduced
in these computations.  This accuracy analysis determines the bounds of
all possible outputs and their associated range of round-off errors for
expressions.

Martel~\cite{martel07} introduces an abstract error semantics for the
calculation of round-off errors in the evaluation of floating-point
expressions.  It consists of three components: an abstract domain to enclose
computed floating-point values and round-off errors, a suite of operations on
the partial order, and finally, a set of arithmetic operations to evaluate
arithmetic expressions in the error semantics.

First we define the domain $\errorset = \floatintervalset\times\intervalset$,
where $\intervalset$ and $\floatintervalset$ respectively represent the
set of real intervals, and the set of floating-point intervals (intervals
that each enclose a range of floating-point values).  The value $(x^\sharp,
\mu^\sharp) \in \errorset$ represents a safe bound on floating-point
values and the accumulated error represented as a range of real values.

Secondly, for the Cartesian product of partial orders, a partial order
relation inherently arises by allowing element-wise partial order
operations~\cite{abramsky94}.  In other words, the following operations for the
partial order relations can be induced for $\errorset$, by using the respective
definitions of the operators in $\floatintervalset$ and $\intervalset$:
\begin{equation}
    \begin{aligned}
        \left( x^\sharp_1, \mu^\sharp_1 \right) \sqsubseteq
        \left( x^\sharp_2, \mu^\sharp_2 \right)
        &\defeq
            x^\sharp_1 \sqsubseteq x^\sharp_2 \wedge
            \mu^\sharp_1 \sqsubseteq \mu^\sharp_2, \\
        \left( x^\sharp_1, \mu^\sharp_1 \right) \sqcup
        \left( x^\sharp_2, \mu^\sharp_2 \right)
        &\defeq
            \left( x^\sharp_1 \sqcup x^\sharp_2,
              \mu^\sharp_1 \sqcup \mu^\sharp_2 \right), \\
        \left( x^\sharp_1, \mu^\sharp_1 \right) \sqcap
        \left( x^\sharp_2, \mu^\sharp_2 \right)
        &\defeq
            \left( x^\sharp_1 \sqcap x^\sharp_2,
              \mu^\sharp_1 \sqcap \mu^\sharp_2 \right),
    \end{aligned}
\end{equation}
for $\left( x^\sharp_1, \mu^\sharp_1 \right) \in \errorset$ and
$\left( x^\sharp_2, \mu^\sharp_2 \right) \in \errorset$.

Furthermore, the error domain forms the following Galois connection:
\begin{equation}
    \powersetof{\floatset\times\realset}
    \galois{}{}
    \floatintervalset\times\intervalset,
\end{equation}

Finally, arithmetic operations can also be defined for the values in
$\errorset$.  To start we introduce the functions $\roundupop$ and
$\rounddownop$, which respectively compute the interval of rounded
floating-point results and the range of round-off error from arithmetic
operations under a given rounding mode.  The function $\roundupop: \intervalset
\to \floatintervalset$ computes the floating-point bound from a real
bound, by rounding the infimum $a$ and supremum $b$ of the input interval
$\interval{a}{b}$:
\begin{equation}
    \roundupop\left(\left[a, b\right]\right)
    \defeq {\left[
        \uparrow_\circ{\left(a\right)},
        \uparrow_\circ{\left(b\right)}
    \right]}_\floatset.
\end{equation}
where the subscript $\floatset$ indicates the interval is a floating-point
interval, and $\uparrow_\circ{\left(v\right)}$ indicates rounding a value
$v$ to a floating-point value.  The function $\rounddownop: \intervalset \to
\intervalset$ determines the range of round-off error due to the floating-point
computation:
\begin{equation}
    \downarrow^\sharp_\circ(\interval{a}{b}) \defeq
        \left[ -\frac{z}{2}, \frac{z}{2}\right], \quad \text{where~}
        z = \max(\ulp(a), \ulp(b)).
\end{equation}
Here $z$ denotes the maximum rounding error that can occur for values
within the range $\interval{a}{b}$, and the unit of the last place ($\ulp$)
function $\ulp(x)$ characterizes the gap between two floating-point numbers
closest to $x$.  However there are multiple different variations of the
definition of $\ulp$, which one is the most fitting is still subject to
debate~\cite{muller}.

We can now define the arithmetic operations on values in $\errorset$.  For
addition, subtraction and unary subtraction, we have:
\begin{equation}
    \begin{aligned}
    \left( x^\sharp_1, \mu^\sharp_1 \right) +
    \left( x^\sharp_2, \mu^\sharp_2 \right)
    &\defeq
        \left(
            \roundup{x^\sharp_1 + x^\sharp_2},
            \mu^\sharp_1 + \mu^\sharp_2 +
            \rounddown{x^\sharp_1 + x^\sharp_2}
        \right), \\
    \left( x^\sharp_1, \mu^\sharp_1 \right) -
    \left( x^\sharp_2, \mu^\sharp_2 \right)
    &\defeq
        \left(
            \roundup{x^\sharp_1 - x^\sharp_2},
            \mu^\sharp_1 - \mu^\sharp_2 +
            \rounddown{x^\sharp_1 - x^\sharp_2}
        \right), \\
    -\left( x^\sharp_1, \mu^\sharp_1 \right)
    &\defeq
        \left( -x^\sharp_1, -\mu^\sharp_1 \right).
    \end{aligned}
    \label{bg:eq:error_arithmetic}
\end{equation}
Multiplication on $\errorset$ can be formulated, by expanding $\left(
x^\sharp_1 + \mu^\sharp_1 \right) \times \left( x^\sharp_2 + \mu^\sharp_2
\right)$ and divide the terms into the multiplied value $x^\sharp_1 \times
x^\sharp_2$ and error terms:
\begin{equation}
    \left( x^\sharp_1, \mu^\sharp_1 \right) \times
    \left( x^\sharp_2, \mu^\sharp_2 \right)
    \defeq
        \left(
            \roundup{x^\sharp_1 \times x^\sharp_2},
            x^\sharp_1 \times \mu^\sharp_2 + x^\sharp_2 \times \mu^\sharp_1 +
            \mu^\sharp_1 \times \mu^\sharp_2 +
            \rounddown{x^\sharp_1 \times x^\sharp_2}
        \right).
    \label{bg:eq:error_arithmetic_mult}
\end{equation}
The addition, subtraction and multiplication of intervals follow the standard
rules of \gls{ia} defined earlier in~\eqref{bg:eq:interval_operations}.

Expressions can be evaluated for their accuracy by the method as follows.
Initially, for real-valued variables, the following function can be used to
convert an interval of real values into a value in the error domain:
\begin{equation}
    \mathrm{cast}\left(x^\sharp\right) \defeq \left(
        \roundup{x^\sharp}, \rounddown{x^\sharp}
    \right).
    \label{bg:eq:cast}
\end{equation}
For example, for the real variable $\vara \in [0.2, 0.3]$ under single
precision with rounding to nearest:
\begin{equation}
    \mathrm{cast}\left([0.2, 0.3]\right) = \left(
        {\left[0.200000003, 0.300000012\right]}_\floatset,
        \left[-1/67108864, 1/67108864\right]
    \right).
\end{equation}

After this, similar to the way we use intervals to analyze a program in
Section~\ref{bg:sub:intervals}, floating-point arithmetic expressions can be
analyzed.  For instance, the expression ${(\vara + \varb)}^2$ can be evaluated
just as we would expect in real arithmetic, instead, the arithmetic operators
are \emph{overridden} to operate on values in the error domain $\errorset$.

For example, assume that real variables $\vara \in [0.2, 0.3]$, $\varb \in
[2.3, 2.4]$, it is possible to derive that in single-precision floating-point
computation with rounding to the nearest, ${(\vara + \varb)}^2 \in [6.24999857,
7.29000187]$ and the error caused by this computation is bounded by
$[-1.60634534\times10^{-6}, 1.60634534\times10^{-6}]$.
