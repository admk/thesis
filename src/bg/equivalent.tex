\section{Discovering Equivalent Programs}
\label{bg:sec:discovering_equivalent_programs}

In this section, we explain existing optimization methods to restructure
numerical programs with arithmetic equivalences.  Because general numerical
programs---consisting of program statements, conditional branches and
loops---supersede arithmetic expressions, we start by introducing optimization
methods of expressions, followed by those of general numerical programs.


\subsection{Improving Performance by Rewriting Arithmetic Expressions}
\label{bg:sub:performance}

\subsubsection{Software Compilers}

It is common knowledge to software programmers that a typical optimizing
compiler, such as \gls{gcc}~\cite{gcc} and Clang~\cite{clang}, has some
traditional static analysis-based optimization passes such as dead code
elimination, loop strength reduction and constant propagation.  These
optimization passes, however, limit themselves by producing implementations
that do not impact numerical accuracy, \ie~they compute the same output given
identical inputs.

A less well-known fact about these compilers is that they also support a
variety of optimization passes that are not enabled by default.  These options,
when enabled, allow the compiler to yield faster software implementations for
programs with a large proportion of floating-point arithmetic computations.
These optimization passes rewrite arithmetic expressions into more efficient
alternative forms which are equivalent to the originals in real arithmetic,
but when executed on a machine they compute different results.  The reason
for the differences is that arithmetic in machines has finite precision,
computed results must be rounded to the nearest representable values.  These
discrepancies, when accumulated, could potentially result in wildly inaccurate
outputs.

There are a number of compiler options in \gls{gcc}~\cite{gcc} that
specifically performs the above optimizations.  For example, the following
options exist to enable expression-rewriting heuristics:
\begin{itemize}

    \item \verb|-fassociative-math| enables arithmetic expression rewriting by
    associativity, one of the heuristics applied is to perform exponentiation
    by squaring, \eg~an expression \verb|x*x*x*x| which requires 3
    multiplications, can be optimized as \verb|(x*x)*(x*x)|, reducing the
    number of multiplications to 2 by sharing the value of the subexpression
    \verb|x*x|;

    \item \verb|-freciprocal-math| can be used to rewrite $x / y$ into $x * (1
    / y)$, if $1 / y$ can be commonly shared among subexpressions; and

    \item \verb|-fno-signed-zeros| ignores the signedness of floating point
    zeros, for example, \verb|0.0f| and \verb|-0.0f| are identical, so that
    \verb|0.0f*x| can be simplified to \verb|0.0| without concerning us with
    the signedness of the result.

\end{itemize}

An extra optimization option, which encompasses the above options, can be used
to enable them all together; it further highlights the \emph{unsafe} nature
of these transformations in its name, \ie~\verb|-funsafe-math-optimizations|.
Besides \gls{gcc}, Clang, which uses the \gls{llvm} framework, provides a
similar option, \verb|-enable-unsafe-fp-math| to use arithmetic equivalences
to reduce the number of floating-point operations in a program, by possibly
sacrificing numerical accuracy.

% To give an example, the value $\sqrt{2}$, being irrational, cannot be
% represented exactly in single-precision floating point, and thus must be
% rounded.

\subsubsection{High-Level Synthesis Tools}

In addition to the unsafe arithmetic expression rewriting heuristics
inherited from the \gls{llvm} framework, \gls{vhls}~\cite{vivado_hls}
and LegUp~\cite{legup}, which are both \gls{llvm}-based, can make use
of hardware-specific optimization passes to allow greater parallelism
in synthesized circuit, thus improving throughput.  The newly designed
transformation to allow data-flow graphs to be restructured to improve
loop parallelism, which allows more computation across loop iterations to
overlap, and subsequently faster programs.  Xilinx's \gls{vhls} has a similar
feature called \emph{expression balancing}~\cite{vivado_hls}, which aims
to balance an arithmetic expression tree using associativity.  Tree height
reduction~\cite{nicolau91} further incorporates distributivity and control-flow
rewriting.  However, neither of these methods produces optimal loop pipelining,
as they do not examine the implications of loop-carried dependences.  For
example, a loop body:
\begin{lstlisting}
    sum = ((sum + A[i]) + B[i]) + C[i];
\end{lstlisting}\vspace{-15pt}
when synthesized in \gls{vhls} with expression balancing, will produce a
schedule which corresponds to the following:
\begin{lstlisting}
    sum = (sum + A[i]) + (B[i] + C[i]);
\end{lstlisting}\vspace{-15pt}
This loop is more efficient than the original, because it has a delay of $2$
adders between consecutive iterations, instead of the original $3$-adder delay,
in the inter-iteration dependences of \verb|sum|.  However, as we will see
below, there is still room for improvement.

Canis~\etal~\cite{canis14} propose a similar approach called \emph{recurrence
minimization}.  They specifically tackle loop pipelining by incrementally
restructuring dependence graphs to minimize longest paths of recurrences.
Their method is subsequently incorporated in LegUp~\cite{legup}.  For instance,
by synthesizing the same original loop in LegUp, it detects that there are
inter-iteration dependences between each pair of \verb|sum| from consecutive
iterations.  The tool will therefore minimize the latency between these
dependences by using associativity to restructure the expression.  This
optimization produces a schedule which corresponds to the following loop body:
\begin{lstlisting}
    sum = sum + ((A[i] + B[i]) + C[i]);
\end{lstlisting}\vspace{-15pt}
It is notable that, by further delaying the addition of \verb|sum|, the loop
now has only a delay of $1$ adder between consecutive iterations.  This
technique can greatly reduce the run time of pipelined loops, especially if the
inter-iteration dependences has a long chain of additions.  However, similar to
\gls{vhls}, they only apply associativity in their restructuring.

\subsubsection{Polynomial Factorization}

The above mentioned tools restrict themselves to simple arithmetic
equivalences, as they are intended for fast and simple optimizations that can
easily be applied by a compiler.  Several techniques take one step further,
by focusing on multivariate polynomials, and factorizing them to minimize
the number of arithmetic operations in an expression.  These approaches are
applicable to both software and hardware designs, as they minimize the number
of operations in an expression, the throughput of the optimized design can be
reduced.  In addition, in an \gls{fpga} circuit, this will also translate to a
reduction of resources utilized.

It is well known that the Horner scheme is the optimal way to
factorize a univariate polynomial so that its operator count is
minimized~\cite{neumaier01}, \eg~a polynomial $x^3 + 2x^2 + 3x + 4$,
which uses 4 multipliers and 3 adders if common subexpressions are
eliminated, can be factorized into $x(x(x + 2) + 3) + 4$, which uses 2
fewer multipliers.  However, a multivariate polynomial could be expressed
in multiple ways using the Horner scheme, and finding the optimal one is
a difficult problem.  Ceberio~\etal~\cite{ceberio04} therefore propose a
greedy algorithm to efficiently factorize a multivariate polynomial to
overcome this.  Hosangadi~\etal~\cite{hosangadi} propose an algorithm for the
factorization of polynomials, in order to eliminate common subexpressions, and
subsequently reduce addition and multiplication counts for a faster software
implementation.  However it is not possible to choose different optimization
levels with their method.  Peymandoust~\etal~\cite{peymandoust} present an
approach that only deals with the factorization of polynomials in \gls{hls}
using \groebner~bases.  The weaknesses of this are its dependence on a set of
library expressions~\cite{hosangadi} and the high computational complexity of
\groebner~bases.

\subsubsection{Shortcomings}

The above approaches utilize a number of heuristics to rewrite expressions, and
do not explore all possible rewrites in full by taking into account additional
equivalence relations.  This could limit their applicability to a small number
of special cases.  The implementation obtained therefore is often likely to
be suboptimal, as further improvements are possible.  More importantly, none
of the above mentioned techniques and tools aim to minimize, or even analyze,
the impact of their transformations on numerical accuracy.  \Gls{hls} tools
therefore generally disable these unsafe features by default for floating-point
computations.  It is in general a good practice to avoid them, if numerical
accuracy is a critical factor to ensure the correctness of the application
being compiled.

% Later we will discuss techniques that can reason about and optimize numerical
% accuracy in programs.

\subsection{Rewriting Arithmetic Expressions for Accuracy}
\label{bg:sub:expression_accuracy}

In many numerically sensitive programs, small round-off errors, when
accumulated, would result in catastrophically inaccurate results.  In
particular, Panchekha \etal~\cite{panchekha15} show that inaccurate
computations, due to round-off errors, resulted in the retraction of scientific
articles~\cite{altman99}, and even wild discrepancies in stock market
indices~\cite{mccullough99}.  In response, the software community has seen an
emergence of techniques that rewrites expressions in numerical programs, to
ameliorate round-off errors in numerically sensitive applications.  However,
round-off errors in numerical programs are well known to be perplexing to
debug~\cite{toronto14}, and it is in general difficult to apply intuitions
manually and rewrite programs to optimize numerical accuracy~\cite{toronto14}.
The numerical accuracy optimization techniques therefore often explore the
search space of equivalent expressions, using equivalence relations in
real arithmetic, ranging from the simplest possible, \eg~associativity,
commutativity and distributivity, to more sophisticated ones, such as
trigonometry facts, rules derived from heuristics $x - y = \frac{x^2 - y^2}{x
+ y}$~\cite{panchekha15}, and many more.

As the vastness of the search space prohibits us to explore it fully,
existing approaches confronting this problem resolve to heuristics.

Darulova~\etal~\cite{darulova} use genetic programming to evolve the structure
of arithmetic expressions into more accurate forms.  However there are several
disadvantages with metaheuristics, such as convergence can only be proved
empirically and scalability is difficult to control because there is no
definitive method to decide how long the algorithm must run until it reaches a
satisfactory goal.

The method proposed by Martel~\cite{martel07} is based on operational semantics
with abstract interpretation, but even their depth limited strategy is, in
practice, at least exponentially complex.

Ioualalen~\etal~\cite{ioualalen}, create a polynomial-size structure, APEG, to
represent an exponential number of equivalent expressions related by rules of
equivalence.  However it restricts itself to only a handful of these rules to
avoid combinatorial explosion of the structure and offers no options for tuning
its optimization level.

Panchekha~\etal~\cite{panchekha15} present a tool, Herbie, which employs a
greedy hill-climbing heuristic to iteratively rewrite expressions in locations
that introduce the largest round-off errors.  Here, detailed overviews are
provided for two distinct approaches, APEG and Herbie.

\subsubsection{APEG}

The APEG proposed by Ioualalen~\etal~\cite{ioualalen} was inspired by
\gls{epeg}, originally introduced by Tate~\etal~\cite{tate09}\footnotemark[1].
\glspl{epeg}, originally intended to discover equivalent structure in programs
but not arithmetic expressions, nevertheless include equivalence rules
that are similar to the ones that exist in real arithmetic.  For instance,
distributivity can by applied over nodes such as $\theta$ and $\mathrm{eval}$
nodes.
\footnotetext[1]{\gls{epeg} and equality saturation is discussed in
Section~\ref{bg:sub:equality_saturation}.}

The APEG is similar in construction to the \gls{epeg}, which is also a
graph-based structure, and promotes the maximal sharing of common subtrees.
The major difference is that because of the lack of control-flow in the
program, APEGs are acyclic, and do not consist of nodes that correspond to
control-flows.  APEGs supersede arithmetic expression by allowing all possible
nodes in an arithmetic expression, \ie~it can contain leaf nodes such as a
constant value or a variable identifier, unary or binary arithmetic operators
that respectively have one or two child nodes.  In addition, to allow an APEG
to encode exponential number of equivalent expressions within a polynomial
space, it further introduces two different kinds of nodes that efficiently
model equivalences.

Firstly, the APEG defines a node containing an \emph{abstraction box},
\fbox{$\opsymbol, \left(p_1, p_2, \mathellipsis, p_n\right)$}\,, where
$\opsymbol$ is a commutative associative operator such as addition $+$ and
multiplication $\times$, and $p_1, p_2, \mathellipsis, p_n$ are the children of
this node.  The abstraction box can be used to represent equivalent expressions
generated by different associative parsings of the expression $p_1 \opsymbol
p_2 \opsymbol \mathellipsis \opsymbol p_n$.  To illustrate, an expression $a
+ b + c$ can be encoded by the abstraction box \fbox{$+, (a, b, c)$}\,, which
can represent all equivalent parsings of the original expression, \ie~$(a
+ b) + c$, $a + (b + c)$ and $(a + c) + b$.  Each abstraction box with $n$
children can represent up to $(2n - 3){!!}$ equivalent expressions without
commutativity~\cite{ioualalen, mouilleron}, as commutativity do not impact
numerical accuracy for commutative operators.

Secondly, in an analogous fashion to the \gls{epeg}, the APEG additionally
admits a new kind of nodes which can be used to enclose a set of equivalent
subexpressions.  Defined as $\langle p_1, p_2, \mathellipsis, p_n \rangle$,
the \emph{equivalent class} is a node which signifies all children subtrees
with root nodes $p_1, p_2, \mathellipsis, p_n$ represent expressions that are
mathematically equivalent.  This is analogous to forming equivalence edges in
\glspl{epeg}.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.4]{bg/fig/apeg.png}
    \caption{%
        An example APEG for the expression $\left( \left( a + a \right) + b
        \right) \times c$, taken from~\cite{martel12}.
    }\label{bg:fig:apeg}
\end{figure}
Take Figure~\ref{bg:fig:apeg} as our example, which not only contains standard
tree nodes which we would expect from an arithmetic expression tree, but
also an abstraction box and equivalent classes.  Each equivalent class is
represented by an ellipse with a dashed border.  It is evident that each
equivalent class indeed reflect a set of equivalent subtrees.  For instance,
consider the bottom equivalent class which has two equivalent subtrees, the
left and right ones represent $2 \times a$ and $a + a$ respectively.  Moreover,
as the subexpression $(a + a) + b$ is a summation of multiple elements, an
abstraction box \fbox{$+, (a, a, b)$}\, can therefore model the equivalent
parsings of it.

Despite the compactness of APEGs, searching for the most accurate
expression within an APEG is still exponentially complex, where the search
utilizes the static analysis of floating-point errors introduced in
Section~\ref{bg:sec:abstract_interpretation}.  For instance, consider an
APEG with $n$ equivalent classes, each contains $2$ different choices, in
the worst case, this may amount to a search of $2^n$ distinct equivalent
expressions, so as to determine the optimal solution.  To resolve this problem,
Ioualalen~\etal~\cite{ioualalen} employs a depth-limited search heuristic to
reduce the size of the search space\footnotemark[2].  Additionally, as it was
explained earlier, the time complexity to search for the optimal parsing of an
abstraction box is double factorial.  They hence make use of another heuristic,
which greedily pairs up terms in an abstraction box $B = \text{\fbox{$+,
\left(p_1, p_2, \mathellipsis, p_n\right)$}}$\,.  To begin, the method searches
for a pair of child subtrees $p_i$ and $p_j$ in $B$, such that the expression
$p_i \opsymbol p_j$ computes with the smallest round-off error.  The nodes
$p_i$ and $p_j$ are subsequently removed from $B$, and a new subtree
\tikz[baseline=-2.3ex]{%
    \newcommand{\edgelength}{3.6ex}
    \node(op) at (0, 0) {\small $\opsymbol$};
    \node(pi) at (-\edgelength, -\edgelength) {\small $p_i$};
    \node(pj) at (\edgelength, -\edgelength) {\small $p_j$};
    \draw(op)--(pi);
    \draw(op)--(pj);
}
is then added to $B$.  The above greedy pairing procedure is then iteratively
repeated until finally one node is left in $B$; this last node is therefore
the expression with an improved numerical accuracy obtained in the process.
\footnotetext[2]{The exact technique is not explained in their original
contribution.}

Ioualalen~\etal~\cite{ioualalen} found their APEG approach can reduce round-off
errors in expressions by up to 50\%.  They further proved the correctness of
their APEG approach by forming a Gal\"ois connection:
\begin{equation}
    \powersetof{{\llparenthesis e \rrparenthesis}_\rhd}
    \galois{\alpha}{\gamma}
    \Pi_\rhd,
\end{equation}
where $\powersetof{{\llparenthesis e \rrparenthesis}_\rhd}$ is all possible
traces of rewrites of an initial expression $e$ using associativity,
distributivity and commutativity, and $\Pi_\rhd$ is the final saturated APEG\@.

\subsubsection{Herbie}

Unlike the APEG approach which focuses on building an \gls{ir} which can
represent exponentially many equivalent expressions, Herbie, introduced by
Panchekha \etal~\cite{panchekha15}, on the other hand, place emphasis on
building an algorithm to efficiently improve numerical accuracy by arithmetic
rewrite rules.

Initially, for a given expression $e$, Herbie samples a randomized set of
input values used to evaluate it, and we use $I$ to denote this set.  In the
optimization process, Herbie analyzes the discrepancy between evaluating
an expression exactly, and in a given floating-point format with precision
$\hat{p}$, due to round-off errors, for a common input $i \in I$.  However,
because expressions contain irrational computations such as square roots,
and transcendental functions, \eg~trigonometry, logarithm and~\etc, exact
computation is generally unattainable as it could require infinite precision
in floating-point.  Instead, Herbie resorts to \gls{mpfr}, and iteratively
increases the precision used to compute the approximate results for $e$, until
the leading 64 bits in the result remain identical across iteration for all
randomly sampled inputs $i \in I$.  Evaluating $e$ in the final precision $p$,
therefore produces an \emph{almost exact} (AE)\footnotemark[3] result for
$e$.  It is notable that in formulae, the glyph $\hat{\,}$ is used to denote
approximation, whereas the absence of $\hat{\,}$ indicates an AE result.
\footnotetext[3]{%
    The original paper~\cite{panchekha15} uses ``exact'' to describe what is in
    fact a floating-point value with a precision $p$, which approximates the
    exact value.  Because this could potentially mislead, ``almost exact'' is
    used here instead.
}

An iterative accuracy refinement procedure is then carried out to optimize an
initial expression $e_0$.  This process identifies operators in the expression
$e_0$ which contribute the largest round-off errors, and specifically apply
arithmetic rewriting to these operators, and repeats the process for the
resulting expressions.  This procedure is carried out for a predetermined
number $N$ of repeats (Herbie uses $N = 3$) as follows.

To begin, for each operator in an original expression $e$, we accumulate all
round-off errors for all sample inputs $i \in I$.  For instance, consider
a binary operator $+$ which accepts two input arguments $e_1$ and $e_2$,
where each argument are subexpressions.  To guarantee the correctness of
the round-off error analysis of $+$, its subexpressions are evaluated
into respective AE values $v_1$ and $v_2$ for a given input $i$.  Using
$v_1$ and $v_2$ as inputs, the operator $+$ is then evaluated twice,
in precision $\hat{p}$ and in AE precision $p$ to produce $\hat{v}_i$
and $v_i$ respectively.  The error between $x = \hat{v}_+$ and $y =
v_+$ is defined as follows~\cite{panchekha15, schkufza14}, which counts
the number of floating-point values between the approximate and exact
results\footnotemark[4]:
\begin{equation}
    \mathcal{E}(x, y) = \log_2 \left| \left\{
        z \in \mathsf{FP} \mid \min(x, y) \leq z \leq \max(x, y)
    \right\} \right|,
\end{equation}
The $\mathcal{E}(\hat{v}_i, v_i)$ is then accumulated for all sample inputs $i
\in I$, and the final sum is the total error measure for the operator $+$, or
equivalently:
\begin{equation}
    \mathrm{Error} = \sum_{i \in I} \mathcal{E}(\hat{v}_i, v_i).
\end{equation}
The top $M$ operators with the largest errors are then selected to be rewritten
in the next stage.  Panchekha~\etal~use $M = 4$ in~\cite{panchekha15}.
\footnotetext[4]{%
    $\mathsf{FP}$ is the set of floating-point values in a precision which was
    not mentioned by the original author~\cite{panchekha15}.
}

The second step is to rewrite the arithmetic expression, using the information
gathered in the previous step about the largest local errors in the expression.
The authors of Herbie identify that if subexpression of the selected operators
are rewritten, transforming the selected operators' expression tree can provide
greater chances of cancelling terms, hence significantly reduce the round-off
errors.  The rewriting process therefore performs a recursively bottom-up
transformation, which first rewrites the smaller subexpressions, then apply
transformation rules to the selected operators.  In contrast to APEGs, Herbie
does not restrict its database of rewrites to the basic arithmetic rules such
as associativity and distributivity, as they also admit expressions with
fractions, square roots, exponentiation, logarithm and trigonometry functions,
additional rules thus must be provided for their corresponding algebraic
identities.  In addition, Herbie has rules for special polynomial patterns,
such as $a^3 + b^3 \equiv (a + b)(a^2 - ab + b^2)$, that are difficult to infer
from using associativity, commutativity and distributivity alone.

The third step is to further simplify the expression generated from the
rewriting step.  In this step, a set of simplification rules are applied to the
expression for a small number of iterations, which transforms the original into
a new expression.  However, unlike step 2, as new expressions are discovered,
an equivalence graph is gradually constructed.  An expression, with the
smallest tree structure, can then be extracted from the graph as the simplified
result.

Herbie further identifies that some of these expressions, when evaluated,
could produce highly inaccurate outcomes near zero or infinity.  In the fourth
step, it will then substitute the expression with a series expansion, which
may no longer suffer from this problem.  This series expansion can be further
restructured in future iterations of refinement.

As the above process produce multiple candidate expressions, in the next
iteration of the accuracy refinement procedure, the four steps above are
applied to all candidate expressions, and these steps will be repeated
$N$ times in total, generating a set of accurate expressions that are
mathematically equivalent to the original.  Finally, because different
candidate expressions may be excel on different input conditions, a dynamic
programming algorithm, is therefore applied to split the input space into
multiple parts, where each part can be computed by different optimized
expressions.  As an example, consider the following formula for finding a root
of the quadratic equation $ax^2 + bx + c = 0$~\cite{panchekha15}:
\begin{equation}
    x_1 = \frac{-b - \sqrt{b^2 - 4ac}}{2a}.
\end{equation}
Their technique can optimize this expression to generate a scheme of
expressions to be evaluated under different conditions, to improve the
numerical accuracy of $x_1$, when evaluated with double-precision:
\begin{equation}
    x_1 = \left\{ \begin{aligned}
            & \frac{4ac}{-b + \sqrt{b^2 - 4ac}} / {2a} \qquad
                && \text{if~} b < 0, \\
            & \left( -b - \sqrt{b^2 - 4ac} \right) \frac{1}{2a}
                && \text{if~} 0 \leq b \leq 10^{127}, \\
            & - \frac{b}{a} + \frac{c}{b}
                && \text{if~} 10^{127} < b.
    \end{aligned} \right.
\end{equation}

Panchekha~\etal~\cite{panchekha15} discovered that their tool can improve the
numerical accuracy of a suite of benchmark examples from~\cite{hamming86},
by recovering up to 60 bits of floating-point precision, with a performance
overhead of 40\% for the generated expressions.


\subsection{Numerical Programs}
\label{bg:sub:numerical_programs}

The technique we have explored so far has only been limited to individual
arithmetic expressions; for a complete numerical program transformation, not
only is it necessary to support sequential execution of straight-line code,
but also control-flow structures such as conditional branches and loops.
However, the techniques in this research area to optimize numerical accuracy
are currently not mature enough to be used in compilers and \gls{hls} tools.

Tate~\etal~\cite{tate09} with their \glspl{epeg}, enable equivalent programs
to be discovered efficiently.  However their method do not consider the
implications of rewriting arithmetic expressions in the data-flow on numerical
accuracy.

Damouche~\etal~\cite{damouche15} developed a new semantics-based method, built
on top of the foundation of APEGs from Ioualalen~\etal~\cite{ioualalen}, for
the purpose of accuracy optimization in general numerical programs.  Their
method formally defines a set of inference rules for rewriting a sequent,
which encode the original program, into another, representing an optimized
program.  To summarize from an informal standpoint, the inference rules
analyze the basic blocks in the numerical program, which consist of arithmetic
operations and variable assignments, into standard arithmetic expressions.
These arithmetic expressions can then be analyzed and optimized by constructing
APEGs and extracting an optimized expression from the APEGs.  Because they
only focus on the basic blocks of the program, containing only data-flows,
their approach is not able to optimize across control-flow boundaries such as
\iflit~statements and \whilelit~loops.  Their method can improve numerical
accuracy by approximately 20\% for simple loop examples.

% The structure of the PEG makes it a difficult task to analyze the graph for
% numerical accuracy, as the $\theta$ node generates an infinite sequence, and
% it is not clear how this sequence can be intelligently pruned to a finite
% sequence for numerical analysis purposes.  The cyclic nature of PEGs further
% aggravates the obstacles of analysis, as often any modifications to a cycle
% would necessitate a local re-analysis.

% \subsection{Word-length Optimization}
% \label{bg:sub:wordlength}

% There are also academic tools that perform precision-performance trade-off by
% optimizing word-lengths of data-paths~\cite{constantinides}; and this safely
% bounds round-off errors.  However, no existing HLS tools can perform the
% trade-off optimization among accuracy, resource usage and latency by varying
% the \emph{structure} of numerical programs.
