The main focus of this thesis is to devise optimization methods to efficiently
rewrite numerical programs by automatically varying the structure of their
control- and data-paths for an improved trade-off between numerical accuracy,
throughput and resource utilization for \gls{hls}\@.  This chapter therefore
provides a review of recent developments in related work and various concepts
that closely relate to the foundation of this central problem.  This section
serves to highlight the main research topic to be discussed in depth in each
section.

We introduce various computing architectures, and explain the pros and cons
of each one in Section~\ref{bg:sec:fpga}.  We then explain in detail how
\glspl{fpga} can be used as an alternative to general-purpose processors
such as \glspl{cpu} and \glspl{gpu} to accelerate computational-intensive
applications, by drawing comparisons of architectural differences to
processors.  This is then followed by an overview of how a traditional
synthesis tool flow compiles \gls{rtl} implementations into circuits by taking
Altera Quartus II~\cite{quartus} as an example.

Because of the ever-increasing complexity of applications designed into
circuits, low-level approaches such as \gls{rtl} design become intractable
for humans~\cite{gajski}, and a gradual adoption of \gls{hls} tools has been
observed in the \gls{fpga} community in recent years.  For this reason, in
Section~\ref{bg:sec:high_level_synthesis}, we review the current state and
advantages of \gls{hls}, and describe the stages employed by these tools to
compile a program in a high-level language into digital circuits.  Because
run time is one of our optimization objectives, we use modulo \gls{sdc}
scheduling to further examine in greater detail how it can make loops to run as
efficiently as possible by pipelining them.  This scheduling algorithm is used
by \gls{hls} tools such as LegUp~\cite{legup}.

The methods discussed in this thesis rely heavily on the static analysis of
numerical programs based on abstract interpretation~\cite{cousot77}.  In
Section~\ref{bg:sec:abstract_interpretation}, we therefore review abstract
interpretation, a theory to create an abstraction for mathematical objects that
are not directly computable---because they are undecidable, intractable or
both---in order to compute an approximation of them efficiently.  We informally
introduce this theory, by way of reviewing it with a progressive program
analysis example.

\Glspl{ir} are often used to facilitate program optimization and program
analysis.  Section~\ref{bg:sec:intermediate} we therefore study in detail
the existing \glspl{ir} designed for program optimization.  We take a closer
look at how the \gls{ssa} and \gls{cfg} approach of \gls{llvm}~\cite{llvm_ir}
encodes a simple program example.  As this technique limits itself to
optimizing a single \gls{ir} of a program, a novel approach known as equality
saturation~\cite{tate09} which uses a data structure to simultaneously
represent multiple optimization candidates, is further examined.

In Section~\ref{bg:sec:discovering_equivalent_programs} we review prior
work on restructuring arithmetic expressions and numerical programs.  We
examine existing optimization passes in software compilers and \gls{hls}
tools.  Since \gls{hls} tools are \gls{llvm}-based and compile C or C++
programs, they borrow the concept from software compiler frameworks, such as
\gls{llvm}, of including optimization passes that may improve performance
of floating-point computations, while potentially sacrificing numerical
accuracies.  On the other hand, we look at methods that could improve
accuracy by simultaneously exploring multiple rewrites of expression
candidates.  Due to the calculation of the full set of equivalent programs
often being computationally intractable, these methodologies either make
extensive use of heuristics, or abstract interpretation techniques introduced
in Section~\ref{bg:sec:abstract_interpretation}, in order to compute an
under-approximation (a potentially smaller set) of the set of all equivalent
programs.

% Section~\ref{bg:sec:wordlength} explores alternative methods, known as
% \emph{word-length optimization}, to trade-off accuracy for other performance
% metrics, for instance, area, throughput and power.  Instead of restructuring
% the arithmetic expression to be synthesized, such as the approaches in
% Section~\ref{bg:sec:discovering_equivalent_programs}, these methods focus on
% varying the precisions of individual arithmetic operators in a data-path, as
% some operators compute values that do not require high precisions and often
% their precisions can often be reduced to optimize other desirable objectives,
% while satisfying a given error budget.
