\section{Thesis Organization}

Chapter~\ref{chp:background} serves to explain various essential concepts used
throughout this thesis.  We introduce rigorous approaches from software static
analysis, specifically formal semantics and abstract interpretation, to bear
on source-to-source transformation for high-level synthesis.  We introduce
a new expression-based intermediate representation to represent numerical
programs, which we call \emph{metasemantic intermediate representation}.  A
MIR is designed to preserve only the effect of program execution, and abstract
away how the program is executed.  For example, details such as temporary
variables and the ordering of program statements are discarded, this hence
greatly reduces the size of the search space, while optimal solutions are
still reachable.  We further introduce the concept of loop pipelining and how
restructuring numerical programs can optimize run time.

In Chapter~\ref{chp:stropt} we propose new methods to automatically optimize
the structure of arithmetic expressions for FPGA implementation as part of
a high-level synthesis flow.  It introduces the basis of the algorithms we
use to perform structural optimization on numerical programs, taking into
account axiomatic rules derived from real arithmetic, such as distributivity,
associativity and others.  A new efficient method is proposed to produce to
generate a computable optimized subset of equivalent expressions from an
original expression.  Our approach explicitly target an optimized area/accuracy
trade-off, by automatically rewriting arithmetic expressions, and analyzing
each expression rewritten for its accuracy and area usage.  This gives
the synthesis tool the flexibility to choose an implementation satisfying
constraints on both accuracy and resource usage.  Using our technique to
optimize the structure of a variety of real world and artificially generated
examples in single precision, we improve either their accuracy or the resource
utilization by up to 60\%.

Chapter~\ref{chp:progopt} presents a similar source-to-source optimization
targeting the trade-off between numerical accuracy and resource usage, but
we have extended it to optimize general numerical programs, consisting of
\iflit~statements and \whilelit~loops.  Because there are infinite number
of ways to rewrite numerical C programs, and many of these rewrites produce
programs that have the same resource usage, accuracy and latency properties, in
this chapter we explain in detail the structure of MIRs, and the back-and-forth
translation between numerical C programs and MIRs.  We efficiently discover
equivalent structures in MIRs by exploiting not only the rules of real
arithmetic, such as associativity and distributivity, but also rules that
enable control flow restructuring.  Our numerical accuracy and resource usage
analyses are further extended to analyze MIRs.  Additionally, we further
broaden the Pareto frontier in our optimization flow to automatically explore
the numerical implications of partial loop unrolling and loop splitting.  In
real applications, the tool discovers a wide range of Pareto optimal options,
and the most accurate one improves the accuracy of numerical programs by up to
65\%.

The optimization techniques flows we have discussed so far have only been
limited to minimize area and round-off errors of numerical programs.  Often
such optimizations result in programs with longer run time, yet there are
potentials for these transformations to significantly improve the run time of
numerical programs.  In Chapter~\ref{chp:latopt}, we therefore introduce an
new analysis procedure to minimize another objective, the total run time of
the optimized program.  Simultaneously with accuracy and resource utilization,
these three forms the independent goals we use to produce the Pareto frontier.
In this chapter, MIRs are extended with new operators to allow for arrays and
matrices in the source program, to optimize a wide-range of real numerical
applications.  Numerical programs typically spend most of their run time in
loops, so state-of-the-art high-level synthesis (HLS) tools use pipelining to
schedule them efficiently.  Still, the run time performance of the resultant
FPGA implementation is limited by data dependences between loop iterations.
We thus present new rewriting rules---memory access reductions---along with
arithmetic identities and control-flow transformations to alleviate some of
these dependence constraints.  HLS tools cannot safely enable such rewrites by
default because they may impact the accuracy of floating-point computations
and increase area usage, whereas we optimize run time while controlling the
implications on accuracy and area.  Again, the tool reports a multi-dimensional
Pareto frontier that the programmer can use to resolve the trade-off according
to their needs.  When applied to a suite of PolyBench and Livermore Loops
benchmarks, the tool has generated programs that enjoy up to a 12$\times$
speedup, with a simultaneous 7$\times$ increase in accuracy, at a cost of up to
4$\times$ more LUTs.

Chapter~\ref{chp:litrev} reviews related work that optimizes programs for
similar goals, or uses similar techniques.  We draw comparisons between others'
work and ours to better motivate the thesis's contributions.

Finally, Chapter~\ref{chp:conclusion} concludes this thesis by summarizing
our research, and we discuss the potentials for how future research into
the structural optimization of numerical programs could further benefit the
high-level synthesis community.
