We believe that it is possible to extend our tool for the multi-objective
optimization of arithmetic expressions in the following ways. First, Secondly,
it would be useful to further allow transformations of expressions while
allowing different mantissa widths in the subexpressions, this further
increases the number options in the Pareto frontier, as well as leads to more
optimized expressions. Thirdly, Finally, .

We are convinced that with the foundation and framework that we developed,
our tool can be extended in the following ways.  First, it can be trivially
extended to support additional numerical data structure such as arrays and
matrices.  Secondly, the Pareto optimization can be extended to optimize the
latencies of equivalent programs, as restructuring programs and partially
unrolling loops could have a notable impact on the ability to pipeline
program loops, especially when arrays are incorporated.  Finally, fixed point
representations, along with the interaction between our structural optimization
and multiple wordlength optimization~\cite{constantinides} could also generate
a lot of interest from the HLS community.

 Also, our tool currently supports only single-precision
floating-point data types; we intend to extend this to multiple-precision
types, and explore the impact on our three performance metrics: latency,
resource utilization and numerical accuracy.


We start by giving an overview of this method.  Equivalent expression discovery
starts from the leaf nodes (these are constants and variables) of expressions,
which equivalent expressions are the nodes themselves, \ie~for a variable or a
constant $x$ its set of equivalent expressions is simply $\{x\}$.  After this,
these equivalent expressions from the child nodes are propagated to the parent
node, to form a set of equivalent parent expressions.  Then a function, which
we name $\closure(\sigma^\sharp)$, is used to perform optimizing transforms
to this set, where $(\sigma^\sharp)$ indicates that this function optimizes
specifically for a program state $\sigma^\sharp \in \errordom$.  It consists
of interleaving iterations of two steps.  The first step is to apply the
transitive relations described in the previous section to discover more
equivalent expressions, however transformations of expressions more than
$k$ nodes deep are disallowed to limit the number of equivalent expressions
discovered.  The constant $k$ is called the \emph{depth limit}.  The next step
is to determine the accuracy and resource usage of all discovered expressions,
and find the Pareto frontier of them to keep only the optimal implementations,
which significantly reduces the computational requirements.  These two steps
are repeated iteratively until no more expressions can be discovered.  After
this, the Pareto frontier of equivalent structures of the current node is
propagated to its parent node to repeat the above process, until the root node
is reached and we arrive at a set of equivalent expressions of the original
expression under optimization.

