\section{Introduction}
\label{po:sec:introduction}

The previous chapter introduced a new methodology to efficiently restructure
arithmetic expressions for the optimized trade-off between two performance
metrics, \ie~numerical accuracy when evaluated and area usage in synthesized
FPGA implementations.  This method however has a substantial limitation when
applied to general numerical programs, that is, it can only be applied to
straight-line codes without control structures such as branches and loops.

In this chapter, a new general \emph{program} optimization technique for
numerical algorithms is therefore proposed, which analyzes and optimizes
\texttt{if} statements as well as \texttt{while} loops in a numerical program.
This enables the joint optimization of accuracy and resource usage, as well as
the trade-off between both performance metrics.  A tool is thus developed to
perform source-to-source optimization of numerical programs targeting FPGAs,
and generate implementations that trade off resource usage and numerical
accuracy.

% floating-point arithmetic has round-off errors, exploit equivalence

Similar to the approach proposed in Chapter~\ref{chp:stropt}, equivalence rules
such as such as \emph{associativity} $(a + b) + c \equiv a + (b + c)$, and
\emph{distributivity} $(a + b) \times c \equiv a \times c + b \times c$ are
exploited to automatically optimize implementations for the optimal trade-off
between resource usage, \ie~the number of LUTs and DSP elements utilized,
and accuracy when evaluated using floating-point computations.  This process
generates a Pareto frontier of optimized numerical programs.  For example, with
single precision floating-point format, the tool found that give an input $x
\in [0, 100]$ and $y \in [0, 2]$, then the program:
\begin{lstlisting}
    if (x < 1) {
        x = (x + y) + 0.1;
    } else {
        x = x + (y + 0.1);
    }
\end{lstlisting}
is most accurate when rewritten in the following form:
\begin{lstlisting}
    if (x < 1) {
        x = (x + 0.1) + y;
    } else {
        x = x + (y + 0.1);
    }
\end{lstlisting}
On the other hand, the original program uses fewest resources when
subexpressions are shared and the \iflit~statement is eliminated:
\begin{lstlisting}
    x = x + (y + 0.1);
\end{lstlisting}

The structural optimization of general numerical programs is much more complex
than that of arithmetic expressions.  The reasons are three-fold.  First,
during program execution, variables are often updated with new values.  Our
optimization should therefore perform static analysis on the values of
variables, and use the result to optimize specifically for the trade-off
between accuracy and resource usage.  Second, it is much more difficult to
formally define program equivalence, and subsequently, to search efficiently
for optimized equivalent programs.  Finally, the combinatorial explosion of
expression equivalence, is further exacerbated by the expressiveness of a
general numerical program.

The difficulty in defining program equivalence is due to the fact that
two programs can be identical in function, but have distinct syntactic
structure because of the expressiveness of a HLL\@.  For instance, even
without the introduction of branches and loops, the following two programs in
Figure~\ref{bg:fig:equiv_progs} are equivalent, but syntactically, they are
very different.
\begin{figure}[ht]
    \centering
    \subfloat[$P_1$]{%
        \shortstack[l]{%
            \texttt{x = x + 1;} \\
            \texttt{y = 2 * x;} \\
            \texttt{x = x + 3;}
        }
    } \qquad \qquad
    \subfloat[$P_2$]{%
        \shortstack[l]{%
            \texttt{y = x + 1;} \\
            \texttt{x = y;} \\
            \texttt{y = y * 2;} \\
            \texttt{x = x + 3;}
        }
    }
    \caption{%
        Two programs that are equivalent but syntactically different.
    }\label{bg:fig:equiv_progs}
\end{figure}

In fact, one can easily imagine infinitely many ways to rewrite numerical
programs, and often these equivalent programs have identical resource usage,
latency and accuracy characteristics.  In practice, it is desirable to
eliminate as much as possible the need for these syntactic rewrites that do not
affect our performance metrics, so that the search space of equivalent programs
is greatly reduced.

We explored in Section~\ref{bg:sec:intermediate} of
Chapter~\ref{chp:background} various intermediate representations designed
for program transformation, and specifically examined PEGs in closer details,
because it fits our requirement to abstract away as much irrelevant syntactic
information as possible.  However, one important shortcoming of the PEG
is that it is not suited for the optimization of numerical accuracy.  By
introducing equivalence edges in the graph, the number of elementary cycles
in the graph could increase exponentially in the number of equivalences that
have been discovered.  Cycles in graphs require analyzing a large proportion
of the IR, whereas a tree structure can be reasoned in a bottom-up hierarchy.
This has significant implications on program transformation.  First, for the
above reason, their approach does not use semantics to optimize for numerical
accuracy, while we wish to reason about semantics and utilize them to steer
optimization.  Moreover, tree structure allows us to easily support partial
loop unrolling by simply extending the equivalence relations while avoiding
re-evaluation.  In contrast, like~\cite{martel09} and~\cite{damouche15},
equality saturation is unable to perform partial loop unrolling.  In this
chapter, a solution to the above limitations of PEGs is therefore proposed,
by introducing a new tree-based intermediate representation with fixpoint
constructs to specifically tackle the problem of program equivalences.

Additionally, as none of the methods in
Section~\ref{bg:sec:discovering_equivalent_programs} of
Chapter~\ref{chp:background} looks at the multiple-objective optimization of
numerical programs, this thesis is the first to propose a tool that performs
a semantics-directed and truly restructuring program transformation, which
optimizes not only arithmetic expressions, but also numerical programs.  The
tool optimizes for the trade-off between numerical accuracy and resource usage
when synthesized to FPGAs.

The program optimization flow is \emph{safe}, \emph{semantics-directed} and
\emph{flexible}. \emph{Safety} means that because we make use of formal
mathematics to optimize programs, our approach can be proved correct, in
the sense that when executed using exact real arithmetic, the transformed
version produces exactly the same output values as the original program.
\emph{Semantics-directed} transformation means that not only do we use
program syntax, but also the semantics to guide optimization and guarantee
safety properties of the optimized program.  Our technique obtains when
necessary, by analyzing the program, a bound and a round-off error bound on
each variable in every program location.  These information are then used
to guide program optimization.  By analyzing and manipulating not only the
syntax, but also the semantics of programs.  The meaning of a \emph{flexible}
program transformation is three-fold.  First, arithmetic computations can be
optimized across assignments, \iflit~statements and \whilelit~loops.  Secondly,
we automatically explore the numerical implications of partial loop unrolling
and loop splitting, which can create more opportunity for minimizing round-off
errors, hence further increases range of options in the Pareto frontier of
trade-offs.  Finally, our method naturally subsumes constant propagation,
redundant code elimination, and also branch and loop fusions.

% Our tool fits in the familiar setting of the high-level synthesis tool flow, as a front-end that performs the source-to-source optimization of

Our main contributions in this chapter are as follows:
\begin{enumerate}
    \vspace{-6pt}
    \item
        A new intermediate representation of the behaviour of numerical
        programs, its structure is designed to be manipulated and analyzed
        with ease.  A new framework of numerical program transformations is
        developed to enable the back and forth translation between the program
        and a new intermediate representation (IR), which preserves the
        semantics of the original program.
    \vspace{-6pt}
    \item
        Semantics-based analyses that reason about not only the resource
        utilization (number of LUTs and DSP elements), and safe ranges of
        values and errors for programs, but also potential errors such as
        overflows and non-termination.
    \vspace{-6pt}
    \item
        A new tool, \newsoap, which trades off resource usage and accuracy
        by providing a safe, semantics-directed and flexible optimization
        targeting numerical programs for high-level synthesis.
    \vspace{-6pt}
\end{enumerate}

This chapter is organized as follows.  We start by defining our program syntax
in Section~\ref{po:sec:syntax_definition}.  Using the syntax definition, we
provide a detailed formal explanation of our numerical program transformation,
which consists of three stages.  Sections~\ref{po:sec:program_to_mir}
describe how how numerical programs can be translated into MIRs,
Section~\ref{po:sec:transformations} discusses how we infer bounds and
error bounds on variables and analyze resource usage estimates, and how
these analyses can be used to efficiently discover equivalent structures
in the analyzed MIR\@.  Section~\ref{po:sec:code_generation} explains how
a chosen MIR can be translated into an optimized numerical program.  Then
we present the optimization results in Section~\ref{po:sec:results} and
Section~\ref{po:sec:conclusion} concludes this chapter.
